<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Lecture &middot; Yu's Tech 블로그
    
  </title>

  <!-- BOOTSTRAP -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <!-- CODE BLOCK FONT -->
  <link href="http://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css" rel="stylesheet" type="text/css">

  <!-- SCROLLSPY-->
  <script src="https://unpkg.com/scrollnav@3.0.2/dist/scrollnav.min.umd.js"></script>
  <link rel="stylesheet" href="/assets/css/style.css">

  
  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

    <!-- syntax.css -->
  <link rel="stylesheet" href="/assets/css/syntax.css">

  <!-- font -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

</head>

  <!--
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>You will never know until you try.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home <small class="text-info">16</small></a>

    

    <!-- 
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/category/Algorithm/index.html">Algorithm</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/category/Deep%20learning/index.html">Deep learning</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="/category/Lecture/index.html">Lecture</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/category/Mongodb/index.html">Mongodb</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/category/OpenCL/index.html">OpenCL</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/category/Operating%20system/index.html">Operating system</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/category/Python/index.html">Python</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/category/Spark/index.html">Spark</a>
        
      
    
      
        
      
    
      
        
      
    
      
    
 -->
    
      <a class = "sidebar-nav-item " href="/category/Python/index.html">Python <small class="text-info">3</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Python/index.html">Python</a>  <small class="text-muted">3</small>-->
    
      <a class = "sidebar-nav-item " href="/category/Mongodb/index.html">Mongodb <small class="text-info">1</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Mongodb/index.html">Mongodb</a>  <small class="text-muted">1</small>-->
    
      <a class = "sidebar-nav-item " href="/category/Spark/index.html">Spark <small class="text-info">1</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Spark/index.html">Spark</a>  <small class="text-muted">1</small>-->
    
      <a class = "sidebar-nav-item " href="/category/Algorithm/index.html">Algorithm <small class="text-info">4</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Algorithm/index.html">Algorithm</a>  <small class="text-muted">4</small>-->
    
      <a class = "sidebar-nav-item " href="/category/Deep learning/index.html">Deep learning <small class="text-info">2</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Deep learning/index.html">Deep learning</a>  <small class="text-muted">2</small>-->
    
      <a class = "sidebar-nav-item " href="/category/OpenCL/index.html">OpenCL <small class="text-info">1</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/OpenCL/index.html">OpenCL</a>  <small class="text-muted">1</small>-->
    
      <a class = "sidebar-nav-item " href="/category/Operating system/index.html">Operating system <small class="text-info">1</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Operating system/index.html">Operating system</a>  <small class="text-muted">1</small>-->
    
      <a class = "sidebar-nav-item  active" href="/category/Lecture/index.html">Lecture <small class="text-info">4</small></a>  
    <!--<a class="sidebar-nav-item" href="/category/Lecture/index.html">Lecture</a>  <small class="text-muted">4</small>-->
    
    
<!--
    <a class="sidebar-nav-item" href="https://github.com/Yujaeseo/Yujaeseo.github.io.git/archive/v1.0.0.zip">Download</a>
    <a class="sidebar-nav-item" href="https://github.com/Yujaeseo/Yujaeseo.github.io.git">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.0.0</span>
-->
  </nav>
<!--
  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
-->
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Yu's Tech 블로그</a>
            <small>프로그래밍 이야기</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
    
    
      <div class="post">
            <h1 class="post-title">
            <a href="/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Installing-tensorflow-serving/">
                Deploying Machine Learning Models in Production_Installing TensorFlow Serving
            </a>
            </h1>

            <span class="post-date">31 Jul 2022</span>
            
            
              
                <span class="badge badge-dark">MLops</span>
              
            
              
                <span class="badge badge-dark">Deep learning</span>
              
            
              
                <span class="badge badge-dark">Machine learning</span>
              
            
            
            <h2 id="install-tensorflow-serving-1"><strong>Install Tensorflow Serving (1)</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110109225.png" alt="image-20220731110109225" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Tensorflow serving을 설치하는 가장 쉬운 방법은 docker image를 사용하는 것이다.</li>
</ul>

<p><br /><br /></p>

<h2 id="install-tensorflow-serving-2"><strong>Install Tensorflow Serving (2)</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110124591.png" alt="image-20220731110124591" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Binaries를 이용해서 직접 다운로드 받는 방법은 다음과 같다.</li>
  <li>Tensorflow-model-server는 플랫폼에 종속된 optimization 방식이 적용된 반면,</li>
  <li>Tensorflow-model-server-universal은 플랫폼에 종속된 optimization 방식이 적용되지 않았다는 점이다.</li>
</ul>

<p><br /><br /></p>

<h2 id="install-tensorflow-serving-3"><strong>Install Tensorflow Serving (3)</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110132863.png" alt="image-20220731110132863" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>직접 customize를 하길 원한다면 source로도 build를 할 수 있다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="install-tensorflow-serving-4"><strong>Install Tensorflow Serving (4)</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110141743.png" alt="image-20220731110141743" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Tensorflow serving을 다운로드 받을 수 있는 archive 위치 정보를 <code class="language-plaintext highlighter-rouge">/etc/apt/sources.list.d</code>에 등록한다.</li>
  <li>
    <p>인증을 위한 key를 불러오고, apt 업데이트 후 <code class="language-plaintext highlighter-rouge">apt-get</code>을 이용하여 tensorflow serving을 다운받는다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="import-the-mnist-dataset"><strong>Import the MNIST Dataset</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110150647.png" alt="image-20220731110150647" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>MNIST 데이터셋을 이용하여 비전 모델을 학습시킨다.
    <ul>
      <li>70,000장 (train 60,000/ test 10,000)의 0-9 grayscale 이미지(28x28)로 이루어져있다.</li>
    </ul>
  </li>
  <li>
    <p>Numpy array 형식으로 데이터를 불러오고 0-1 scale 값으로 normalization을 수행한다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="import-the-mnist-dataset-1"><strong>Import the MNIST Dataset</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110209791.png" alt="image-20220731110209791" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>Train, test 각각을 (60,000 x 28 x 28 x 1), (10,000 x 28 x 28 x 1) 형식의 array로 변환한다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="look-at-a-sample-image"><strong>Look at a Sample Image</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110217800.png" alt="image-20220731110217800" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>Sample image를 확인한다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="build-a-model"><strong>Build a Model</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110226735.png" alt="image-20220731110226735" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>간단한 CNN 모델을 구성한다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="train-the-model"><strong>Train the Model</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110234568.png" alt="image-20220731110234568" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>모델 training에 사용할 optimizer, loss, metric을 세팅한다.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Fit</code>을 실행하면 모델 학습이 진행되고, history 변수에는 epoch by epoch accuracy가 저장된다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="evaluate-the-model"><strong>Evaluate the Model</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110241758.png" alt="image-20220731110241758" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Testset에 대해 학습된 모델의 정확도를 평가한다.
<br /><br /></li>
</ul>

<h2 id="save-the-model"><strong>Save the Model</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110250111.png" alt="image-20220731110250111" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>Tensorflow serving을 사용하기 위해서는 모델을 저장해야한다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="launch-your-saved-model"><strong>Launch Your Saved Model</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110257655.png" alt="image-20220731110257655" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>bash script로 tensorflow model server를 세팅해준다. (port, 모델 이름, 모델 path)</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="send-an-inference-request"><strong>Send an Inference Request</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110306735.png" alt="image-20220731110306735" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>Test set에 속하는 몇개의 이미지를 <code class="language-plaintext highlighter-rouge">json</code> 형식으로 변환한후 <code class="language-plaintext highlighter-rouge">POST</code>로 서버에 request를 보낸다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="plot-predictions"><strong>Plot Predictions</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110314567.png" alt="image-20220731110314567" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>결과를 시각화하는 코드를 작성한다.</p>

    <p><br /><br /></p>
  </li>
</ul>

<h2 id="results-demo"><strong>Results Demo</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110323919.png" alt="image-20220731110323919" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>결과가 정확하게 나온 것을 확인할 수 있다.</li>
</ul>

<p><br /><br /></p>

        </div>
  
      <div class="post">
            <h1 class="post-title">
            <a href="/lecture/2022/07/30/Deploying-Machine-Learning-Models-in-Production-Deployment-Options/">
                Deploying Machine Learning Models in Production_Deployment Options
            </a>
            </h1>

            <span class="post-date">30 Jul 2022</span>
            
            
              
                <span class="badge badge-dark">MLops</span>
              
            
              
                <span class="badge badge-dark">Deep learning</span>
              
            
              
                <span class="badge badge-dark">Machine learning</span>
              
            
            
            <h2 id="model-deployments"><strong>Model Deployments</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095805422.png" alt="image-20220730095805422" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>모델 배포는 데이터센터와 같이 대규모의 인프라 공간에 모델을 중앙집중형 서버에 배포하는 것과 각 사용자의 local device에 배포하는 두가지가 있다.</li>
</ul>

<p><br /><br /></p>

<h2 id="running-in-huge-data-centers"><strong>Running in Huge Data Centers</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095619218.png" alt="image-20220730095619218" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>데이터센터를 운영하면 cost를 줄이기 위한 노력을 수행한다.
    <ul>
      <li>Google과 같은 큰 기업들은 지속적으로 데이터센터의 resource utilization, application의 cost를 줄이려고 한다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="constrained-environment-mobile-phone"><strong>Constrained Environment: Mobile Phone</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095627775.png" alt="image-20220730095627775" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>거대한 데이터센터 뿐만 아니라 핸드폰과 같은 제한된 모바일 환경에서도 모델을 serving하기도 한다.</li>
  <li>모바일핸드폰의 GPU에는 데이터센터에서 사용되는 GPU보다 크기가 훨씬 작고 메모리 용량은 일반적으로 4GB가 넘지 않는다.</li>
  <li>이러한 GPU를 우리가 배포한 application이 점유하는 것이 아니라 실행되고 있는 여러 application이 점유하게 된다.</li>
  <li>또한 GPU를 이용하여 연산을 가속화하면 배터리가 빨리 닳게 되고 열이 발생해 application에 대한 인식이 좋지 않아질 것이다.</li>
  <li>마지막으로, application의 용량이 커지면 다운로드 받기 꺼려질 것이다.</li>
</ul>

<p><br /><br /></p>

<h2 id="restrictions-in-a-constrained-environment"><strong>Restrictions in a Constrained Environment</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095635328.png" alt="image-20220730095635328" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>낮은 메모리 용량, 프로세싱 파워, 배터리 용량과 같은 제한된 요건을 갖기 때문에 edge device에 큰 모델을 배포할 수 없다.</li>
  <li>이러한 이유로 보통 서버에 모델을 배포하고 REST API를 통해 서비스를 하지만, latency가 매우 중요할 때는  적합하지 않다.
    <ul>
      <li>자율주행자동차는 실시간으로 그때그때 판단을 해야 하는데, 서버와 통신을 하게 되면 network 지연과 같은 문제로 큰 문제가 초래될 수 있다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="prediction-latency-is-almost-always-important"><strong>Prediction Latency is Almost Always Important</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095647690.png" alt="image-20220730095647690" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>항상 latency 를 줄여 application의 response time을 줄여 user experience를 향상시켜라.
    <ul>
      <li>예외적인 경우는 prediction의 정확도가 훨씬 중요한 task를 수행할 때이다.</li>
      <li>예시는 질병 분석이 있다.</li>
    </ul>
  </li>
  <li>이때 model complexity, size와 같은 trade-off 관계에 있는 요소들도 고려해야 하며, 발생하는 cost도 고려해야 한다.</li>
</ul>

<p><br /><br /></p>

<h2 id="choose-best-model-for-the-task"><strong>Choose Best Model for the Task</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095656008.png" alt="image-20220730095656008" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>언급한 constraints를 기준으로 최적의 모델을 선택해야한다.
    <ul>
      <li>Mobilenet은 모바일 환경을 위해 개발된 모델이다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="other-strategies"><strong>Other Strategies</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095702751.png" alt="image-20220730095702751" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>프로파일링을 통해 병목지점을 파악하고,</li>
  <li>특정 operation이 많은 시간을 차지한다는 것을 발견한다면, 해당 부분은 최적화해야 한다.</li>
  <li>모델 자체를 최적화하여 더 빠르고 energy efficient한 모델을 만들수도 있며 특히 mobile 환경에서 중요하다.</li>
  <li>사용하는 thread의 수를 늘리는 방법도 있다.
    <ul>
      <li>하지만 thread 수를 늘린다고 해서 무조건 성능이 좋아지는 것은 아니다.</li>
      <li>무엇을 concurrent하게 실행하고 있는지 등에 따라 성능 차이가 발생한다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="web-applications-for-users"><strong>Web Applications for Users</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095709919.png" alt="image-20220730095709919" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>모델에서 서비스를 받는 유저는 request를 web application을 통해한다.</li>
  <li>Model은 API 서비스 형태로 존재하고, 이러한 과정을 돕는 다양한 web framework들이 존재한다.</li>
</ul>

<p><br /><br /></p>

<h2 id="serving-systems-for-easy-deployment"><strong>Serving systems for easy deployment</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095716511.png" alt="image-20220730095716511" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>모델 서빙 시스템은 다양한 방식으로 모델 배포를 관리한다.
    <ul>
      <li>서버에 모델을 배포하여 서비스를 제공할 수 있으며,</li>
      <li>Custom website가 필요하지 않고,</li>
      <li>몇줄 안되는 코드로 배포가 가능하고,</li>
      <li>Model update와 rollback을 편리하게 수행할 수 있다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="clipper"><strong>Clipper</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095723575.png" alt="image-20220730095723575" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Clipper는 UC berkely에서 만든 model serving 시스템이다.
    <ul>
      <li>다양한 모델 배포를 지원하며, restful API로 기존 application과 쉽게 통합할 수 있으며, 도커로 컨테이너화하여 자원 관리를 지원하며, latency setting을 할 수 있다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="tensorflow-serving"><strong>TensorFlow Serving</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095732320.png" alt="image-20220730095732320" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Tensorflow serving은 구글에서 만든 오픈소스이다.
    <ul>
      <li>텐서플로우 모델을 쉽게 배포할 수 있으며,</li>
      <li>다른 타입의 모델도 배포할 수 있으며,</li>
      <li>REST와 gRPC 프로토콜을 제공하며,</li>
      <li>모델의 version management가 가능하다.</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="advantages-of-serving-with-a-managed-service"><strong>Advantages of Serving with a Managed Service</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095739374.png" alt="image-20220730095739374" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Google cloud와 같은 managed service를 사용하면 더 편할 수 있다.
    <ul>
      <li>낮은 latency의 endpoint를 제공하고 대량의 batch를 처리할 수 있으며,</li>
      <li>별도 환경 또는 클라우드에서 학습한 모델을 배포할 수 있으며,</li>
      <li>traffic에 기반하여 자동으로 scaling을 수행하고</li>
      <li>GPU/TPU와 같은 연산 가속기도 지원한다.
        <ul>
          <li>Google 외에도 MS, Amazon과 같은 기업들이 비슷한 서비스를 제공한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

        </div>
  
      <div class="post">
            <h1 class="post-title">
            <a href="/lecture/2022/07/29/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving-Infrastructure/">
                Deploying Machine Learning Models in Production_Introduction to Model Serving Infrastructure
            </a>
            </h1>

            <span class="post-date">29 Jul 2022</span>
            
            
              
                <span class="badge badge-dark">MLops</span>
              
            
              
                <span class="badge badge-dark">Deep learning</span>
              
            
              
                <span class="badge badge-dark">Machine learning</span>
              
            
            
            <hr />

<p><br /><br /></p>

<h2 id="optimizing-models-for-serving"><strong>Optimizing Models for Serving</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225850339.png" alt="image-20220729225850339" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>최근 모델의 정확도를 높이기 위해 model에 feature 수를 늘리는 등 모델 자체의 complexity가 커지고 있다.</p>
  </li>
  <li>
    <p>정확도는 높아지지만 이로인해 prediction latency도 커지고 있다.</p>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="as-model-complexity-increases-cost-increases"><strong>As Model Complexity Increases Cost Increases</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225858482.png" alt="image-20220729225858482" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>모델 complexity의 증가는 비용을 초래한다.</li>
  <li>GPU, TPU와 같은 하드웨어와 model registry, maintenance에 있어서의 비용이 포함된다.</li>
</ul>

<p><br /><br /></p>

<h2 id="balancing-cost-and-complexity"><strong>Balancing Cost and Complexity</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225907449.png" alt="image-20220729225907449" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>모델의 정확도와 prediction 속도 간에는 trade-off 관계가 존재하며, 이 사이의 균형을 맞추는 것이 중요하다.</li>
</ul>

<p><br /><br /></p>

<h2 id="optimizing-and-satisficing-metrics"><strong>Optimizing and Satisficing Metrics</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225915162.png" alt="image-20220729225915162" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>모델의 정확도를 측정하는 metric으로 accuracy, precision, recall이 있다.</p>
  </li>
  <li>
    <p>반면, 연산상의 제약 조건과 관련된 metric으로는 latency, model size, GPU load가 있다.</p>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="optimizing-and-satisficing-metrics-1"><strong>Optimizing and Satisficing Metrics</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225927745.png" alt="image-20220729225927745" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>이와 같은 metric을 만족하는 방법은 우선 특정한 serving infrastructure에서 model의 complexity를 증가시키는 것이다.</p>
  </li>
  <li>
    <p>Latency와 같은 연산 상의 제약조건 metric threshold에(e.g. latency) 걸리는 순간까지 정확도를 높인다.</p>
  </li>
  <li>
    <p>이후 최종 결과를 평가하고 난 후에 정확도를 높이거나, infra를 증축하거나, 모델의 complexity를 줄이거나와 같은 대책을 마련하고 실행한다.</p>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="use-of-accelerators-in-serving-infrastructure"><strong>Use of Accelerators in Serving Infrastructure</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225935034.png" alt="image-20220729225935034" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>Accelerator를 통해 infrastructure를 최적화할 수 있다.</p>
  </li>
  <li>
    <p>GPU의 경우는 training 가속화에 강점을 보이며,</p>
  </li>
  <li>
    <p>TPU는 large batch size, complex model inference에 강점이 있다.</p>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="maintaining-input-feature-lookup"><strong>Maintaining Input Feature Lookup</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225942225.png" alt="image-20220729225942225" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Prediction을 수행할 때에는 많은 feature들이 필요할 수 있다.</li>
  <li>예를들어 음식 배달 시간을 예측한다고 할때, 주문 수, 현재 교통 상황, 처리되지 않은 주문 수와 같은 많은 정보들이 필요하다.</li>
  <li>Prediction latency를 줄이기 위해서는 사용되는 대량의 정보를 data store에서 빠르게 읽을 수 있어야 한다.</li>
  <li>이때 cache를 사용하면 필요한 정보를 빠르게 읽어 latency를 낮출 수 있다.</li>
</ul>

<p><br /><br /></p>

<h2 id="nosql-databases-caching-and-feature-lookup"><strong>NoSQL Databases: Caching and Feature Lookup</strong></h2>

<p><img src="/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225950122.png" alt="image-20220729225950122" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>NoSQL은 caching과 feature look up을 구현하기에 좋으며, 위와 같이 다양한 옵션들이 존재한다.</li>
</ul>

<p><br /><br /></p>

        </div>
  
      <div class="post">
            <h1 class="post-title">
            <a href="/lecture/2022/07/28/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving/">
                Deploying Machine Learning Models in Production_Introduction to Model Serving
            </a>
            </h1>

            <span class="post-date">28 Jul 2022</span>
            
            
              
                <span class="badge badge-dark">MLops</span>
              
            
              
                <span class="badge badge-dark">Deep learning</span>
              
            
              
                <span class="badge badge-dark">Machine learning</span>
              
            
            
            <hr />

<p><br /><br /></p>

<h2 id="what-exactly-is-serving-a-model"><strong>What exactly is Serving a Model?</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/what%20exactly%20is%20Serving%20a%20Model.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>전체 ML 프로젝트에서 model training은 우리가 아는 굉장히 일부분이다.</li>
  <li>그중 model serving은 학습시킨 model을 end user가 사용할 수 있도록 하는 것이며,</li>
  <li>이를 위해서는 end user가 interaction할 수 있는 app 또는 서비스가 필요하다.</li>
</ul>

<p><br /><br /></p>

<h2 id="model-serving-patterns"><strong>Model Serving patterns</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Model%20Serving%20patterns.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>Inference는 모델에 입력 값을 넣고 예측을 하는 과정이며,</p>
  </li>
  <li>
    <p>크게 세가지 (model, interpreter, input data)가 필요하다.</p>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="ml-workflows"><strong>ML workflows</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/ML%20workflows.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>ML workflow는 크게 <strong>model training</strong>, <strong>model prediction</strong> 두 가지로 나뉜다.</p>
  </li>
  <li>
    <p>이때, model training은 <strong>offline learning(batch learning, static learning)</strong>과 <strong>online learning(dynamic learning)</strong>으로 나눌 수 있다.</p>

    <ul>
      <li><strong>Offline learning</strong>
        <ul>
          <li>Offline learning은 일정 기간 동안 이미 수집된 데이터를 바탕으로 특정 시기에 모델 학습을 시키는 것이다.</li>
          <li>즉 기존 data와 새로 들어온 data를 함께 학습시키게 되며 대량의 데이터를 한번에 처리하기 때문에 학습시 많은 시간과 자원이 소요된다.</li>
          <li>모델을 배포하면 재학습시까지 모델은 고정된 상태가 되며, 오랜 시간이 지나면 새로운 pattern에 대응하지 못하며 <strong>model decay</strong>가 발생한다.</li>
        </ul>
      </li>
      <li><strong>Online learning</strong>
        <ul>
          <li>Online learning 은 실시간으로 스트림 데이터가 들어올 때 마다 주기적으로 모델을 학습시키는 것이다.</li>
          <li>주로 sensor, stock 데이터와 같은 time-series data을 토대로 학습을 할때 많이 활용한다.
<br /><br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Model prediction도 <strong>Batch prediction, Realtime prediction</strong> 두 타입이 존재한다.</p>

    <ul>
      <li><strong>Batch prediction</strong>
        <ul>
          <li>Batch prediction은 과거에 모인 다량의 데이터를 기반으로 예측을 하는것이다.</li>
          <li>즉 한번의 예측에서 많은 수의 인스턴스를 처리하게 된다.</li>
          <li>데이터가 time dependent하지 않고, realtime으로 예측을 하는 것이 중요하지 않은 상황에 적절하다.</li>
        </ul>
      </li>
      <li><strong>Realtime prediction</strong>
        <ul>
          <li>Real time prediction은 inference 요청이 온 시점에 들어온 데이터를 기반으로 실시간으로 prediction을 하는 것이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="important-metrics"><strong>Important metrics</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Important%20metrics.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Online inference에서 중요하게 살펴보는 metric으로는 latency, throughput, cost가 있다.</li>
</ul>

<p><br /><br /></p>

<h2 id="lantency"><strong>Lantency</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Lantency.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Latency는 user의 요청과 이에대한 application의 응답까지의 시간을 의미한다.</li>
</ul>

<p><br /><br /></p>

<h2 id="throughput"><strong>Throughput</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Throughput.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Throughput은 단위 시간동안 처리한 요청의 수를 의미한다.</li>
</ul>

<p><br /><br /></p>

<h2 id="cost"><strong>Cost</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Cost.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Serving infrastructure는 반드시 cost를 수반하게 된다.</li>
</ul>

<p><br /><br /></p>

<h2 id="minimizing-latency-maximizing-throughput"><strong>Minimizing Latency, Maximizing Throughput</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Minimizing%20Latency,%20Maximizing%20Throughput.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>
    <p>많은 기업들은 latency를 최소화하며, throughput을 최대화하길 원한다.</p>
  </li>
  <li>
    <p>예를들어 항공사 추천 서비스가 있다고 할때, 유저의 요청에 빠르게 응답할 수 있어야하며, 공휴일과 같이 유저가 몰리는 시간에는 많은 요청을 빠르게 처리할 수 있어야 한다.</p>
  </li>
  <li>
    <p>이러한 요건을 충족시키기 위해서는 infrastructure를 확장해야 하는데, 이때 큰 비용이 발생한다.</p>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="balance-cost-latency-and-throughput"><strong>Balance Cost, Latency and Throughput</strong></h2>

<p><img src="/assets/img/Lecture/Introduction%20to%20Model%20Serving/Balance%20Cost,%20Latency%20and%20Throughput.png" alt="image" width="100%" height="100%" class="align-center" /></p>

<ul>
  <li>Infrastructure 확장에 많은 비용을 투자하지 않고 이러한 비용을 낮추기 위한 방법이 있다.
    <ul>
      <li>GPU 자원 공유</li>
      <li>Multi model serving</li>
      <li>Model optimization</li>
    </ul>
  </li>
</ul>

        </div>
  

</div>
<div class="pagination">
    
      <span class="pagination-item newer">New</span>
    
  
    
      <span class="pagination-item older">Old</span>
    
  
  </div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  </body>
</html>

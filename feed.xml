<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-09-06T17:28:39+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yu’s Tech 블로그</title><subtitle>You will never know until you try.</subtitle><author><name>Jaeseo Yu</name></author><entry><title type="html">Docker를 이용한 deep learning 개발 환경 구축</title><link href="http://localhost:4000/docker/2022/09/04/Docker-deep-learning-dev-environment/" rel="alternate" type="text/html" title="Docker를 이용한 deep learning 개발 환경 구축" /><published>2022-09-04T00:00:00+09:00</published><updated>2022-09-04T00:00:00+09:00</updated><id>http://localhost:4000/docker/2022/09/04/Docker%20deep%20learning%20dev%20environment</id><content type="html" xml:base="http://localhost:4000/docker/2022/09/04/Docker-deep-learning-dev-environment/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;개요&quot;&gt;&lt;strong&gt;개요&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;이번 글은 딥러닝을 위한 개발 환경을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker&lt;/code&gt;를 이용해 구축하는 일부 과정을 담았다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ubuntu&lt;/code&gt;에 딥러닝 개발에 많이 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anaconda&lt;/code&gt; 프레임워크를 포함하는 개발 환경을 구축하기 위한 dockerfile을 작성하고, 이를 토대로 docker image build, container 실행까지 해본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dockerfile-작성&quot;&gt;&lt;strong&gt;Dockerfile 작성&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dockerfile&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;은  docker image를 만들기 위한 command들이 모여있는 text 파일이다. Dockerfile의 파일 이름은 별도의 확장자 없이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;이라고 설정하는 것이 권장된다. 따라서, 프로젝트 디렉토리에 파일을 생성한 후 아래의 내용을 참고해서 dockerfile을 작성하면된다. 파일을 작성한 후에는 해당 파일을 build하여 image를 생성하고, 이를 실행하면 container를 생성한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Base image 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ubuntu:18.04&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Image를 만들때는 작업의 효율성을 위해 보통 다른 사람들이 많들어 놓은 image를 기반으로 그 위에 하나씩 layer를 쌓게된다. 만약 이미지의 모든 부분을 스스로 만든다고 하면 모든 dependecy 및 setting을 고려하여 작업을 해야하기 때문에 이미지 제작에 소요되는 시간이 무척 오래 걸릴 것이다.  AI를 전공하는 사람들이 가장 많이 사용하는 linux 운영체제 중 하나가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt;이기 때문에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; 이미지를 base image로 설정한다. 이때 Base image는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; 명령어를 통해 설정할 수 있다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; 뒤에 원하는 이미지와 태그명을 쓰면 된다. 이후에 base image 위에 anaconda를 구동하기 위한 환경 설정을 하고 anaconda를 install한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Shell 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SHELL&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; , &quot;-o&quot;, &quot;pipefail&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Image build 시에 사용할 shell을 변경하려면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHELL&lt;/code&gt; 명령어를 사용해야 한다. 사용 형태는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHELL [&quot;executable&quot;, &quot;parameters&quot;]&lt;/code&gt;으로 해당 명령어는 기존에 설정된 shell을 override 할수 있게 해준다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHELL&lt;/code&gt; 명령어는 파일 내에서 여러번 나올 수 있는데, 이때는 나중에 나온 설정이 이전 설정을 override하게 된다.&lt;/p&gt;

    &lt;p&gt;Linux의 default shell은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/sh&lt;/code&gt;이지만, 현재 anaconda는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/sh&lt;/code&gt;를 지원하지 않으므로, ubuntu에서 일반적으로 많이 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash&lt;/code&gt;를 default로 세팅해준다. 이때  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--login&lt;/code&gt; 옵션으로  login shell을 실행하도록 설정한다.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-c&lt;/code&gt;  인자는 string에서 command를 읽도록 설정하는 옵션이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-o pipefail&lt;/code&gt; 옵션은 이후에 user 설정에서 실행할 pipeline을 위해 넣어준 옵션으로서, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt;를 이용해서 연결된 command를 실행할 때 도중에 error가 발생하면 이를 시스템과 사용자가 발견할 수 있도록해준다. 해당 설정을 사용하지 않으면, pipeline 실행의 return 값이 마지막에 실행된 command에서 나오게 된다. 따라서 마지막 command만 정상적으로 수행된다면 이전에 발생한 error를 발견할 수 없다. 반면에 해당 옵션을 사용하면, return 값이 오른쪽으로 전달되어 가장 오른쪽에서 발생한 error가 pipeline 실행 결과 값으로 나오게되고 해당 return 값을 통해 오류 발생 여부를 판단할 수 있게된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;apt-get 업데이트 및 wget 설치&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;RUN  &lt;/span&gt;apt-get update &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; wget &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /var/lib/apt/lists/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Anaconda installer를 서버에 설치해야 되는데, 이때 ubuntu에서 웹서버에서 프로그램을 다운로드 할때  주로 사용되고 있는&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wget&lt;/code&gt;를 이용하려고 한다. 이를 위해서 패키지 관리 툴인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt(Advanced Package Tool)&lt;/code&gt;가 install, upgrade, cleaning를 위해 제공하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;을 업데이트 한 후에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wget&lt;/code&gt;을 install 한다. 필요한 패키지를 설치한 후에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt&lt;/code&gt;가 관리하는 패키지에 대한 정보들이 저장되어 있는 저장소(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/apt/lists&lt;/code&gt;)를 비워 이미지 사이즈를 줄인다.  추가적으로 프로그램 실행 권한을 관리하는 명령어인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt;도 설치한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Non-root user 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;기본적으로 docker container는 root 권한을 갖고 실행되도록 세팅이 되어있다. 이때 container 안에서 실행되는 application들도 root 권한을 갖게 된다. 이때 만약 해커가 호스트에서 실행되는 container 및 application을 해킹하면 호스트 시스템의 root 권한을 얻어 여러 resource에 접근할 수 있게 되어 보안상 큰 문제가 발생할 수 있게된다. 따라서, 일반적으로 container를 non-root user로 실행하여 발생할 수 있는 보안상의 문제를 완화할 필요가 있다.&lt;/p&gt;

    &lt;p&gt;하지만, 현재 생성하고자 하는 docker container는 딥러닝 개발을 위해 개인적으로 사용하려고 하는 목적이기 때문에 user를 생성하되, sudo 권한을 부여하여 컨테이너 설정 및 이용을 편하게 할수 있도록 한다.&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; username=ubuntu&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uid=11&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; upwd=123&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; USER=$username UID=$uid UPWD=$upwd HOME=/home/$username&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;useradd &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-G&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /bin/bash &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$UID&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$UPWD&lt;/span&gt; | chpasswd
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'%sudo ALL=(ALL) NOPASSWD:ALL'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/sudoers
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $HOME&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt;는 docker image build시에 사용할 수 있는 값들을 설정할 때 쓰이는 값이다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENV&lt;/code&gt;와의 차이는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt;에서 설정한 값은 image build 시점에서만 사용되지만, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENV&lt;/code&gt;에서 설정한 값은 image build 시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt;command와 build 후 container runtime에서 사용될 수 있다는 점이다. 두 명령어를 통해 user 정보와 관련된 값들을 설정하고, 이 값들을 유저를 추가할 수 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;useradd&lt;/code&gt;  명령어와 함께 사용하여 sudo 권한을 갖는 유저를 생성한다. 이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/sudoers&lt;/code&gt;에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'%sudo ALL=(ALL) NOPASSWD:ALL'&lt;/code&gt;를 추가하여 계정 password를 치지않고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt; 명령어를 사용할 수 있게 설정한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Anaconda 설치&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; URL_PREFIX=https://repo.anaconda.com/miniconda \&lt;/span&gt;
	INSTALLER_URL=$URL_PREFIX/Miniconda3-latest-Linux-x86_64.sh \
	CONDA_DIR=~/miniconda3
     	
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;wget &lt;span class=&quot;nt&quot;&gt;--quiet&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$INSTALLER_URL&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/miniconda.sh &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	&lt;span class=&quot;nb&quot;&gt;chmod &lt;/span&gt;u+x ~/miniconda.sh &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	~/miniconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; ~/miniconda.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;전 과정에서 설치한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wget&lt;/code&gt;으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;miniconda&lt;/code&gt;를 설치한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;miniconda&lt;/code&gt;는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anaconda&lt;/code&gt;와 다르게 최소한의 패키지만을 포함하는 압축된 버전이다. 기본적인 요구 사항들만을 포함하고 있기 때문에 원하는 패키지를 그때그때 다운받아야하지만, 가볍고 빠르다는 장점이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conda 기본 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; PATH=$CONDA_DIR/bin:$PATH&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;conda init bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Bash에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda&lt;/code&gt; 명령어 사용을 위한 환경 변수 설정과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda init bash&lt;/code&gt; 를 통해 bash에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt;  명령어를 비롯하여 conda에서 제공하는 다양한 기능을 사용할 수 있도록 설정해준다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Project directory 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; PROJECT_DIR ~/app&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PROJECT_DIR&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $PROJECT_DIR&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;진행하고자하는 프로젝트에 필요한 소스코드, 소프트웨어를 비롯한 파일을 모아둘 폴더(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app&lt;/code&gt;)를 home directory(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/&lt;/code&gt;)에 생성한다. 프로젝트 별로 필요한 여러 파일들을 하나의 directory에 저장하고 관리하면 깔끔하고 관리의 용이성도 커진다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conda environment 생성&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Conda environment를 이전 과정에서 만든 directory에 생성한다. 이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda create&lt;/code&gt;명령어를 사용하며 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt;  인자를 통해 가상환경을 생성할 폴더의 path(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ENV_PREFIX/test&lt;/code&gt;)를 지정한다. 가상환경이 생성은 됐지만, 가상환경의 이름은 지정되지 않은 상황이다. 따라서 가상환경을 지칭할 때 full path를 입력해야된다. 하지만 가상환경을 활성화하는 것을 비롯하여 가상환경을 이용할 때 이를 지칭하는 이름이 있어야 명령어가 짧아지고 관리하기 편해진다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda config --append&lt;/code&gt; 명령어에 가상환경 폴더(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ENV_PREFIX/test&lt;/code&gt;)의 parent directory (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ENV_PREFIX&lt;/code&gt;)를 넘겨주면, conda configuration 파일인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.condarc&lt;/code&gt; 에 가상환경 directory를 추가할 수 있으며, 이를 통해가상환경을 지칭하는 이름을 설정할 수 있다.&lt;/p&gt;

    &lt;p&gt;가상환경 생성, 이름 세팅을 별도의 명령어로 나누어서 진행했지만, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda create&lt;/code&gt; 에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-n&lt;/code&gt; 인자를 통해 가상환경 생성시에 이름을 설정할 수 있다. 다만, 구체적인 이유는 모르겠지만 가상환경 폴더의 path를 설정하는 인자(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt;)와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-n&lt;/code&gt;를 함께 쓸 수 없기 때문에 차선책으로 가상환경을 지정된 위치에 생성, 이름을 지정하는 두번의 step으로 나누어서 진행하였다.&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ENV_PREFIX $PROJECT_DIR/env&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;conda update &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; base &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; defaults conda &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	conda create &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;/test &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	conda config &lt;span class=&quot;nt&quot;&gt;--append&lt;/span&gt; envs_dirs &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	conda clean &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--yes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Container 실행 명령 정의&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/bin/bash&quot;, &quot;-l&quot;, &quot;-i&quot;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;USER&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $USER&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Entrypoint&lt;/code&gt; 명령어를 이용하여 컨테이너 시작시 실행할 명령을 지정할 수 있다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash&lt;/code&gt;를 login shell로 실행하도록 설정하며,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USER $USER&lt;/code&gt;를 통해 root가 아닌 생성한 user로 컨테이너를 사용할수 있도록 세팅한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;image-build-및-container-실행&quot;&gt;&lt;strong&gt;Image build 및 container 실행&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Build image&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-powershell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;docker&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$UID&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;upwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$UPWD&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_TAG&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;작성한 Dockerfile을 이용하여 image를 생성하기 위해서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt; command를 사용한다. Dockerfile이 존재하는 디렉토리 경로를 넘겨주게 되면, 자동으로 Dockerfile을 읽어 image를 생성한다.이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tag&lt;/code&gt; 옵션을 통해 이미지 이름과 tag 정보를 추가할 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--build-arg&lt;/code&gt; 를 이용하면 build 시에 Dockerfile 내의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt;의 대응되는 key 값에 사용자가 넣은 값을 할당할 수 있게된다.  계정 패스워드와 같은 민감한 정보를 이와 같은 식으로 넘겨주는 것은 피해야하며 docker에서 제공하는  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secret&lt;/code&gt;과 같은 기능을 사용하여 정보를 넘겨주는 것이 좋다. 하지만 이번 프로젝트의 목적은 개인을 위한 딥러닝 개발 환경을 시험삼아 한번 만들어 보는 것이기 때문에 이와 같은 사항을 고려하지 않기로 한다. 참고로 필자는 window powershell에서 프로젝트를 진행하기 때문에, backslash(\)가 아닌 backtick(`)을 사용하여 command의 행을 나눴다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Run container&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-powershell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;docker&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_TAG&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; 명령어와 이미지 이름 및 태그 정보를 통해 생성한 docker 이미지를 container로 만들 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-it&lt;/code&gt; 옵션은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--interactive&lt;/code&gt; ,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tty&lt;/code&gt;가 결합된 것으로, 컨테이너가 바로 종료되지 않고 현재 작업하고 있는 terminal에서 CLI를 통해 컨테이너를 조작할 수 있도록 만들어준다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;결과&quot;&gt;&lt;strong&gt;결과&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;container를 실행하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anaconda&lt;/code&gt;가 설치된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; 환경에 접속할 수 있게 되고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda env list&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt;를 통해 원하는 가상환경이 제대로 설치됐는지 확인하고 실행해볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220906162127088.png&quot; alt=&quot;image-20220906162127088&quot; /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Docker" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="docker" /><summary type="html">Ubuntu, anaconda 기반 딥러닝 개발 환경을 docker를 이용해서 직접 구축해본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Servers-Model Servers_Other Providers</title><link href="http://localhost:4000/lecture/2022/08/09/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Model-Servers_Other-Providers/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Servers-Model Servers_Other Providers" /><published>2022-08-09T00:00:00+09:00</published><updated>2022-08-09T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/08/09/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Model%20Servers-Model%20Servers_Other%20Providers</id><content type="html" xml:base="http://localhost:4000/lecture/2022/08/09/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Model-Servers_Other-Providers/">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;NVIDIA Triton Inference Server&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202430432.png&quot; alt=&quot;image-20220808202430432&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Triton Inference Server는 NVIDIA에서 제공하는 model server이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;오픈소스 inference 서빙 플랫폼으로서 tensorflow, pytorch, ONNX 등 타 딥러닝 프레임워크에서 학습된 모델들도 deploy를 할수 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Architecture&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202555416.png&quot; alt=&quot;image-20220808202555416&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;서로 다른 framework에서 만들어진 모델을 single GPU에서 concurrently하게 실행할 수 있다.&lt;/li&gt;
      &lt;li&gt;Multi GPU 환경에서는 각 모델에 대한 instance가 생성되며 각 GPU에 할당된다.
        &lt;ul&gt;
          &lt;li&gt;User의 별도 coding 없이 GPU utilization을 높일 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch inference와 streaming input에 대한 inference도 지원한다.&lt;/li&gt;
      &lt;li&gt;Performance를 높이기 위해 inference input, output을 CUDA GPU shared memory에 적재할 수도 있다.
        &lt;ul&gt;
          &lt;li&gt;httpc, grpc overhead를 줄일 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Designed for Scalability&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202603559.png&quot; alt=&quot;image-20220808202603559&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Autoscaling, orchestration을 위해 K8s와 결합되었고, end-to-end ML workflow를 위해 Kubeflow와도 결합된다.&lt;/li&gt;
      &lt;li&gt;Http, grpc API를 지원하며 이를 통해 load balancer 통신할 수 있다.&lt;/li&gt;
      &lt;li&gt;수많은 모델들이 동시에 serving될 수 있으며, GPU 뿐만 아니라 model을 heterogeneous한 환경에서 돌릴 수 있다.
        &lt;ul&gt;
          &lt;li&gt;Peak load 시에는 여러 프로세서 자원에 load를 분산시켜 효율적으로 대응할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Torch Serve&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202611455.png&quot; alt=&quot;image-20220808202611455&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Torchserve는 AWS와 Facebook이 함께 만들었으며, Pytorch model을 위한 serving framework이다.&lt;/li&gt;
      &lt;li&gt;여러 기능을 제공하며, 무엇보다 opensource여서 자신에게 fit하게 customize할 수 있다는 장점이 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (1)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202619768.png&quot; alt=&quot;image-20220808202619768&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Torchserve의 아키텍쳐는 다음과 같습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (2)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202628376.png&quot; alt=&quot;image-20220808202628376&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Frontend는 request와 response 처리와 model lifecycle을 관리한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (3)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202636889.png&quot; alt=&quot;image-20220808202636889&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Backend는  model store로 부터 load된 model instance를 실행하며, inference를 담당한다.&lt;/li&gt;
      &lt;li&gt;여러 model worker가 실행중인 것을 볼 수 있으며, 동일 model에 대한 여러 worker , 또는 서로 다른 model에 대한 여러 worker가 실행될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (4)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202645785.png&quot; alt=&quot;image-20220808202645785&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;모델은 cloud 또는 local storage로 부터 읽을 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (5)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202654281.png&quot; alt=&quot;image-20220808202654281&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Server는 management, inference를 위한 API 또한 제공한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KFServing&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202703639.png&quot; alt=&quot;image-20220808202703639&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Kubeflow 또난 KF serving을 통한 serving 서비스를 제공하며, 여러 framework에서 만들어진 모델 serving을 지원한다.&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Tensorflow Serving외에 Triton Inference Server 등 다른 server에 대해 알아본다.</summary></entry><entry><title type="html">NVIDIA GPUDirect storage에 대해</title><link href="http://localhost:4000/hpc/2022/08/07/NVIDIA-GPUDirect-storage/" rel="alternate" type="text/html" title="NVIDIA GPUDirect storage에 대해" /><published>2022-08-07T00:00:00+09:00</published><updated>2022-08-07T00:00:00+09:00</updated><id>http://localhost:4000/hpc/2022/08/07/NVIDIA%20GPUDirect%20storage</id><content type="html" xml:base="http://localhost:4000/hpc/2022/08/07/NVIDIA-GPUDirect-storage/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-gpudirect&quot;&gt;&lt;strong&gt;NVIDIA GPUDirect&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA GPUDirect란 NVIDIA data center GPU의 data movement와 access 속도를 높여주는 기술이다. AI를 위해 많은 기업들이 고성능 GPU 서버를 구축하고 있고, 높은 flops를 가지는 GPU의 성능을 활용하기 위해서는 높은 IO bandwidth가 뒷받침되어야 한다. 아래의 이미지는 NVIDIA 기술 세미나에서 인용한 HP의 슬라이드이다. Computer architecture 각 구성 요소들의 처리 속도를 사람의 걸음수로 환산하여 비교한 자료이다. CPU 프로세서의 처리 속도를 한 걸음이라고 했을 때, system memory (DRAM)와 SSD (Flash)의 IO 속도를 각각 100, 200,000 걸음으로 환산할 수 있다. CPU 처리 속도에 비해 주변 장치의 IO 속도가 훨씬 느리기 때문에 processor의 빠른 처리 속도를 십분 활용하기 위해서 IO 속도를 높이는 것이 무엇보다 중요하다고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220807223108967.png&quot; alt=&quot;image-20220807223108967&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 1. Computer architecture (&lt;a href=&quot;https://www.youtube.com/watch?v=ZMf64oB_arY&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NIVIDIA GPU의 빠른 처리 속도를 활용해 시스템 성능을 극대화하기 위해 GPUDirect라는 기술을 개발하였고, storage access 최적화를 위한 GPUDirect Storage, network adapter access 최적화를 위한 GPUdirect RDMA, GPU 간 통신 최적화를 위한 GPUDirect P2P를 비롯한 다양한 기술들이 포함되어 있다. 그중 스토리지와의 data movement 속도를 높여주는 NVIDIA GPUDirect Storage(GDS)에 대해 알아보고자 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-gpudirect-storage-gds&quot;&gt;&lt;strong&gt;NVIDIA GPUDirect storage (GDS)&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GDS는 local 또는 remote storage(e.g. NVMe or NVMe over Fabric)와 GPU 간 direct path를 이용하여 빠르게 통신할 수 있도록 해주는 기술이다. 이를 통해 IO 시 CPU에서 발생하는 bottleneck을 줄여줄 수 있게된다. GDS가 없을 때는 local 또는 remote storage에서 데이터를 불러올 때 CPU를 거쳐야만 했다. 즉 CPU의 low memory에 있는 bounce buffer에 storage에서 읽은 데이터가 쌓이고, 해당 데이터를 high memory에 copy한 후 해당 데이터가 GPU 메모리로 이동하는 방식이였다. 반면에 GDS를 사용하면 아래의 그림과 같이 CPU를 거치지 않고 storage와 GPU간에 direct하게 통신할 수 있게 된다. GPU와 storage 간의 DMA (Direct Memory Access)를 통해 CPU의 불필요한 memory copy를 줄일 수 있게 되며, 통신을 위해 거치는 data path가 짧아져 data transfer에 소요되는 시간이 줄게된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/GPUDirect Storage data path.png&quot; alt=&quot;GPUDirect Storage data path&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 2. GPUDirect Storage data path (&lt;a href=&quot;https://nvdam.widen.net/s/k8vrp9xkft/tech-overview-magnum-io-1790750-r5-web&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;아래의 그래프는 NVIDIA 자체 gdsio benchmarking tool을 이용해서 bandwidth와 cpu utilization을 측정한 자료이다. 자료에 따르면 GDS를 사용하지 않았을 때(CPU-GPU_READ)에 비해 bandwidth는 최대 1.5X 향상되었으며, CPU utilization의 경우 최대 2.8X 향상되었음을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/The benefits of using GDS with the gdsio benchmarking tool.png&quot; alt=&quot;The benefits of using GDS with the gdsio benchmarking tool&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 3. The benefits of using GDS with the gdsio benchmarking tool (&lt;a href=&quot;https://nvdam.widen.net/s/k8vrp9xkft/tech-overview-magnum-io-1790750-r5-web&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Weather pattern을 확인하는 climate simulation deep learning 모델에 GDS, DALI (Nvidia data loading library), deepCAM를 적용하여 inference 성능을 측정했을 때 최대 6.6X의 성능 향상이 있었다. (GDS만 적용했을 때의 성능 개선치만 볼 수 있었다면 좋았을 것 같다).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/Performance benchmarking done for DeepCAM Inference.png&quot; alt=&quot;Performance benchmarking done for DeepCAM Inference&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 4. Performance benchmarking done for DeepCAM Inference using standard GDS configuration in DGX A100 (&lt;a href=&quot;https://nvdam.widen.net/s/k8vrp9xkft/tech-overview-magnum-io-1790750-r5-web&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="HPC" /><category term="nvidia" /><category term="gpu" /><category term="hpc" /><category term="deep learning" /><category term="machine learning" /><summary type="html">GPU와 storage 간 IO 퍼포먼스를 향상시키는 NVIDIA GPUDirect Storage에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Servers-TensorFlow Serving</title><link href="http://localhost:4000/lecture/2022/08/02/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Tensorflow-Serving/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Servers-TensorFlow Serving" /><published>2022-08-02T00:00:00+09:00</published><updated>2022-08-02T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/08/02/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Model%20Servers-Tensorflow%20Serving</id><content type="html" xml:base="http://localhost:4000/lecture/2022/08/02/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Tensorflow-Serving/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving1&quot;&gt;&lt;strong&gt;Tensorflow Serving(1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230627297.png&quot; alt=&quot;image-20220801230627297&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving은 tensorflow model을 바로 서빙할 수 있으며, Non TF model도 serving이 가능하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving2&quot;&gt;&lt;strong&gt;Tensorflow Serving(2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230635154.png&quot; alt=&quot;image-20220801230635154&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다량의 request를 동시에 처리하는 batch inference 기능을 제공한다.
    &lt;ul&gt;
      &lt;li&gt;Recommendation engine에서 주로 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;단일 request에 대해 빠른 inference 결과를 제공하는 real-time inference 기능도 제공한다.
    &lt;ul&gt;
      &lt;li&gt;Image classification task에서 주로 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving3&quot;&gt;&lt;strong&gt;Tensorflow Serving(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230643875.png&quot; alt=&quot;image-20220801230643875&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 task에대해 여러 모델을 serving하는 것도 가능하다.
    &lt;ul&gt;
      &lt;li&gt;A/B test에 유용하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving4&quot;&gt;&lt;strong&gt;Tensorflow Serving(4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230657706.png&quot; alt=&quot;image-20220801230657706&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rest endpoint도 제공한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture1&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230706249.png&quot; alt=&quot;image-20220801230706249&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High level architecture는 이와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture2&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230714137.png&quot; alt=&quot;image-20220801230714137&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TF serving은 TF servable을 중심으로 만들어졌으며, TF servable은 추상화된 객체로서 client가 inference나 lookup과 같은 연산을 수행할 때 사용한다.&lt;/li&gt;
  &lt;li&gt;전형적인 servable은 model이나 lookup table이 될수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture3&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230722617.png&quot; alt=&quot;image-20220801230722617&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loader로 TF servable의 lifecycle을 관리한다.&lt;/li&gt;
  &lt;li&gt;Loader API는 일반적인 인프라와 머신러닝 알고리즘, 데이터를 비롯한 태스크와 independent하게 만들어 준다.&lt;/li&gt;
  &lt;li&gt;Loader API로 servable을 load, unload 할수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture4&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230729609.png&quot; alt=&quot;image-20220801230729609&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source는 loader를 통해 여러 set으로 이루어진 서로 다른 version의 servable이 manager에게 전달한다.&lt;/li&gt;
  &lt;li&gt;새로운 servable이 전달되면 기존의 것은 unload한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture5&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(5)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230736712.png&quot; alt=&quot;image-20220801230736712&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이처럼 manager는 servable의 lifecycle을 통제하며, servable을 loading하거나 list에 없는 servable을 unload할 수 있다.&lt;/li&gt;
  &lt;li&gt;새로운 stream이 들어오면 version policy에 따라 load, unload한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture6&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(6)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230744753.png&quot; alt=&quot;image-20220801230744753&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Servable handler는 client가 servable을 이용할 수 있도록 interface를 제공한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture7&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(7)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230752000.png&quot; alt=&quot;image-20220801230752000&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;전반적인 프로세스를 이해하기 위해 예시를 들어보자.&lt;/li&gt;
  &lt;li&gt;Source가 빈번하게 weight이 update되는 tensorflow graph라고 가정하자.&lt;/li&gt;
  &lt;li&gt;이러한 weight은 filesystem에 저장이된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture8&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(8)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230759184.png&quot; alt=&quot;image-20220801230759184&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model의 weight가 바뀌었다고 할때, source는 새로운 버전의 모델을 감지한다.&lt;/li&gt;
  &lt;li&gt;디스크에 있는 모델 데이터의 pointer를 담은 loader를 만든다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture9&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(9)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230805817.png&quot; alt=&quot;image-20220801230805817&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source는 dynamic manager한테 이러한 version을 전달한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture10&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(10)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230818129.png&quot; alt=&quot;image-20220801230818129&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic manger는 version policy에 따라 새로운 version을 load한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture11&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(11)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230830401.png&quot; alt=&quot;image-20220801230830401&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;메모리가 충분하면, loader는 manager의 요청에의해 변경된 weight이 반영된 tensorflow graph를 servable로 instance화한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture12&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(12)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230838113.png&quot; alt=&quot;image-20220801230838113&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Client는 새로운 버전에 대한 handle을 요청하면 manager는 servable에 대한 handle을 return한다.&lt;/li&gt;
  &lt;li&gt;Client는 해당 servable을 이용하여 inference를 수행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-참고자료&quot;&gt;&lt;strong&gt;# 참고자료&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;아키텍처 전반과 용어에 대한 설명이 적혀있는 참고자료
    &lt;ul&gt;
      &lt;li&gt;https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/architecture.md&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model server 중 하나인 tensorflow serving에 대한 아키텍처를 살펴본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Serving Architecture</title><link href="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Model-Serving-Architecture/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Serving Architecture" /><published>2022-07-31T00:00:00+09:00</published><updated>2022-07-31T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/31/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Model%20Serving%20Architecture</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Model-Serving-Architecture/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-infrastructure-1&quot;&gt;&lt;strong&gt;ML Infrastructure (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165510452.png&quot; alt=&quot;image-20220731165510452&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model serving을 위한 infrastructure에 대해 알아볼 것이다.&lt;/li&gt;
  &lt;li&gt;크게 자체 구축한 on-premise 환경 또는 cloud 서비스를 이용하는 두가지 방법이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-infrastructure_on-premise-2&quot;&gt;&lt;strong&gt;ML Infrastructure_On-Premise (2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165518094.png&quot; alt=&quot;image-20220731165518094&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On-Premise는 소유한 회사가 하드웨어와 소프트웨어에 대한 전적인 통제 권한을 가질 수 있다.&lt;/li&gt;
  &lt;li&gt;따라서, 시시각각 변하는 다양한 요구 조건에 빠르게 대처할 수 있다는 장점이 존재한다.&lt;/li&gt;
  &lt;li&gt;하지만 하드웨어 인프라를 직접 구하고, 설치하고, 유지보수해야한다는 점에서 큰 비용이 생긴다.&lt;/li&gt;
  &lt;li&gt;일반적으로 규모의 경제를 실현할 수 있는 큰 규모의 회사들이 on-premise 환경을 구축하고 운영한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-infrastructure_cloud-3&quot;&gt;&lt;strong&gt;ML Infrastructure_Cloud (3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165526447.png&quot; alt=&quot;image-20220731165526447&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;규모가 작은 기업들은 이러한 인프라를 전문 기업에게 outsourcing 하는 방법이 있다.&lt;/li&gt;
  &lt;li&gt;아마존, 구글, 마이크로소프트와 같은 기업들이 cloud 서비스를 제공한다.&lt;/li&gt;
  &lt;li&gt;하드웨어를 직접 구축할 필요가 없고 해당 기업들이 하드웨어, 소프트웨어 전반을 관리 및 모니터링해준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving-1&quot;&gt;&lt;strong&gt;Model Serving (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165534694.png&quot; alt=&quot;image-20220731165534694&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인프라 선택뿐만 아니라 model serving 방법 또한 선택해야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving_on-premise-2&quot;&gt;&lt;strong&gt;Model Serving_On-Premise (2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165542350.png&quot; alt=&quot;image-20220731165542350&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On-premise에서는 원하는 model server를 선택하고 구축할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving_cloud3&quot;&gt;&lt;strong&gt;Model Serving_Cloud(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165549647.png&quot; alt=&quot;image-20220731165549647&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloud를 사용하는 경우 cloud vendor에서 제공하는 tool과 service를 사용할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;Google의 경우 automl을 비롯한 다양한 서비스를, amazon의 경우 sagemaker와 같은 서비스를 사용할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아니면 cloud에 VM을 활용하여 on-premise와 동일하게 직접 원하는 환경과 서비스를 구축할수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers-1&quot;&gt;&lt;strong&gt;Model Servers (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165556703.png&quot; alt=&quot;image-20220731165556703&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model server의 역할을 간단하게 알아보자.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_model-file2&quot;&gt;&lt;strong&gt;Model Servers_Model file(2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165604454.png&quot; alt=&quot;image-20220731165604454&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델은 여러 버전으로 file system에 저장이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_model-server3&quot;&gt;&lt;strong&gt;Model Servers_Model server(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165612567.png&quot; alt=&quot;image-20220731165612567&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model server는 model을 file에서 읽고 인스턴스로 생성하여, client에게 제공하길 원하는 task에 대한 response를 한다.&lt;/li&gt;
  &lt;li&gt;예를들어 Mobilenet을 이용한 classification을 수행한다고 했을 때, 모델 서버는 input으로 주어진 이미지를 tensor 형태로 변형하고 이를 model에 전달하여 inference 결과를 산출한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_api4&quot;&gt;&lt;strong&gt;Model Servers_API(4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165624486.png&quot; alt=&quot;image-20220731165624486&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model server는 client에게 REST 또는 gRPC 형태의 API를 제공하고 inference 결과를 client에게 return한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_popular-model-server5&quot;&gt;&lt;strong&gt;Model Servers_Popular model server(5)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165635639.png&quot; alt=&quot;image-20220731165635639&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;유명한 model server에는 Tensorflow serving, Torchserve, KF serving, Triton이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving에 필요한 infrastructure의 종류와 model server에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Installing TensorFlow Serving</title><link href="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Installing-tensorflow-serving/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Installing TensorFlow Serving" /><published>2022-07-31T00:00:00+09:00</published><updated>2022-07-31T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/31/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Installing%20tensorflow%20serving</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Installing-tensorflow-serving/">&lt;hr /&gt;

&lt;h2 id=&quot;install-tensorflow-serving-1&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110109225.png&quot; alt=&quot;image-20220731110109225&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving을 설치하는 가장 쉬운 방법은 docker image를 사용하는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-tensorflow-serving-2&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110124591.png&quot; alt=&quot;image-20220731110124591&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Binaries를 이용해서 직접 다운로드 받는 방법은 다음과 같다.&lt;/li&gt;
  &lt;li&gt;Tensorflow-model-server는 플랫폼에 종속된 optimization 방식이 적용된 반면,&lt;/li&gt;
  &lt;li&gt;Tensorflow-model-server-universal은 플랫폼에 종속된 optimization 방식이 적용되지 않았다는 점이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-tensorflow-serving-3&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110132863.png&quot; alt=&quot;image-20220731110132863&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;직접 customize를 하길 원한다면 source로도 build를 할 수 있다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;install-tensorflow-serving-4&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110141743.png&quot; alt=&quot;image-20220731110141743&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving을 다운로드 받을 수 있는 archive 위치 정보를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/apt/sources.list.d&lt;/code&gt;에 등록한다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;인증을 위한 key를 불러오고, apt 업데이트 후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;을 이용하여 tensorflow serving을 다운받는다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;import-the-mnist-dataset&quot;&gt;&lt;strong&gt;Import the MNIST Dataset&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110150647.png&quot; alt=&quot;image-20220731110150647&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MNIST 데이터셋을 이용하여 비전 모델을 학습시킨다.
    &lt;ul&gt;
      &lt;li&gt;70,000장 (train 60,000/ test 10,000)의 0-9 grayscale 이미지(28x28)로 이루어져있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Numpy array 형식으로 데이터를 불러오고 0-1 scale 값으로 normalization을 수행한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;import-the-mnist-dataset-1&quot;&gt;&lt;strong&gt;Import the MNIST Dataset&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110209791.png&quot; alt=&quot;image-20220731110209791&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train, test 각각을 (60,000 x 28 x 28 x 1), (10,000 x 28 x 28 x 1) 형식의 array로 변환한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;look-at-a-sample-image&quot;&gt;&lt;strong&gt;Look at a Sample Image&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110217800.png&quot; alt=&quot;image-20220731110217800&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sample image를 확인한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;build-a-model&quot;&gt;&lt;strong&gt;Build a Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110226735.png&quot; alt=&quot;image-20220731110226735&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;간단한 CNN 모델을 구성한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-the-model&quot;&gt;&lt;strong&gt;Train the Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110234568.png&quot; alt=&quot;image-20220731110234568&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모델 training에 사용할 optimizer, loss, metric을 세팅한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fit&lt;/code&gt;을 실행하면 모델 학습이 진행되고, history 변수에는 epoch by epoch accuracy가 저장된다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluate-the-model&quot;&gt;&lt;strong&gt;Evaluate the Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110241758.png&quot; alt=&quot;image-20220731110241758&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Testset에 대해 학습된 모델의 정확도를 평가한다.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;save-the-model&quot;&gt;&lt;strong&gt;Save the Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110250111.png&quot; alt=&quot;image-20220731110250111&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tensorflow serving을 사용하기 위해서는 모델을 저장해야한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;launch-your-saved-model&quot;&gt;&lt;strong&gt;Launch Your Saved Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110257655.png&quot; alt=&quot;image-20220731110257655&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;bash script로 tensorflow model server를 세팅해준다. (port, 모델 이름, 모델 path)&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;send-an-inference-request&quot;&gt;&lt;strong&gt;Send an Inference Request&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110306735.png&quot; alt=&quot;image-20220731110306735&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Test set에 속하는 몇개의 이미지를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; 형식으로 변환한후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST&lt;/code&gt;로 서버에 request를 보낸다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;plot-predictions&quot;&gt;&lt;strong&gt;Plot Predictions&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110314567.png&quot; alt=&quot;image-20220731110314567&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;결과를 시각화하는 코드를 작성한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-demo&quot;&gt;&lt;strong&gt;Results Demo&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110323919.png&quot; alt=&quot;image-20220731110323919&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;결과가 정확하게 나온 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Google colab을 이용하여 tensorflow serving을 install하고 간단한 CNN 모델을 통해 실습을 진행한다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Deployment Options</title><link href="http://localhost:4000/lecture/2022/07/30/Deploying-Machine-Learning-Models-in-Production-Deployment-Options/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Deployment Options" /><published>2022-07-30T00:00:00+09:00</published><updated>2022-07-30T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/30/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Deployment%20Options</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/30/Deploying-Machine-Learning-Models-in-Production-Deployment-Options/">&lt;hr /&gt;

&lt;h2 id=&quot;model-deployments&quot;&gt;&lt;strong&gt;Model Deployments&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095805422.png&quot; alt=&quot;image-20220730095805422&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델 배포는 데이터센터와 같이 대규모의 인프라 공간에 모델을 중앙집중형 서버에 배포하는 것과 각 사용자의 local device에 배포하는 두가지가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-in-huge-data-centers&quot;&gt;&lt;strong&gt;Running in Huge Data Centers&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095619218.png&quot; alt=&quot;image-20220730095619218&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터센터를 운영하면 cost를 줄이기 위한 노력을 수행한다.
    &lt;ul&gt;
      &lt;li&gt;Google과 같은 큰 기업들은 지속적으로 데이터센터의 resource utilization, application의 cost를 줄이려고 한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;constrained-environment-mobile-phone&quot;&gt;&lt;strong&gt;Constrained Environment: Mobile Phone&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095627775.png&quot; alt=&quot;image-20220730095627775&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;거대한 데이터센터 뿐만 아니라 핸드폰과 같은 제한된 모바일 환경에서도 모델을 serving하기도 한다.&lt;/li&gt;
  &lt;li&gt;모바일핸드폰의 GPU에는 데이터센터에서 사용되는 GPU보다 크기가 훨씬 작고 메모리 용량은 일반적으로 4GB가 넘지 않는다.&lt;/li&gt;
  &lt;li&gt;이러한 GPU를 우리가 배포한 application이 점유하는 것이 아니라 실행되고 있는 여러 application이 점유하게 된다.&lt;/li&gt;
  &lt;li&gt;또한 GPU를 이용하여 연산을 가속화하면 배터리가 빨리 닳게 되고 열이 발생해 application에 대한 인식이 좋지 않아질 것이다.&lt;/li&gt;
  &lt;li&gt;마지막으로, application의 용량이 커지면 다운로드 받기 꺼려질 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;restrictions-in-a-constrained-environment&quot;&gt;&lt;strong&gt;Restrictions in a Constrained Environment&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095635328.png&quot; alt=&quot;image-20220730095635328&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;낮은 메모리 용량, 프로세싱 파워, 배터리 용량과 같은 제한된 요건을 갖기 때문에 edge device에 큰 모델을 배포할 수 없다.&lt;/li&gt;
  &lt;li&gt;이러한 이유로 보통 서버에 모델을 배포하고 REST API를 통해 서비스를 하지만, latency가 매우 중요할 때는  적합하지 않다.
    &lt;ul&gt;
      &lt;li&gt;자율주행자동차는 실시간으로 그때그때 판단을 해야 하는데, 서버와 통신을 하게 되면 network 지연과 같은 문제로 큰 문제가 초래될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;prediction-latency-is-almost-always-important&quot;&gt;&lt;strong&gt;Prediction Latency is Almost Always Important&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095647690.png&quot; alt=&quot;image-20220730095647690&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;항상 latency 를 줄여 application의 response time을 줄여 user experience를 향상시켜라.
    &lt;ul&gt;
      &lt;li&gt;예외적인 경우는 prediction의 정확도가 훨씬 중요한 task를 수행할 때이다.&lt;/li&gt;
      &lt;li&gt;예시는 질병 분석이 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이때 model complexity, size와 같은 trade-off 관계에 있는 요소들도 고려해야 하며, 발생하는 cost도 고려해야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;choose-best-model-for-the-task&quot;&gt;&lt;strong&gt;Choose Best Model for the Task&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095656008.png&quot; alt=&quot;image-20220730095656008&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;언급한 constraints를 기준으로 최적의 모델을 선택해야한다.
    &lt;ul&gt;
      &lt;li&gt;Mobilenet은 모바일 환경을 위해 개발된 모델이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;other-strategies&quot;&gt;&lt;strong&gt;Other Strategies&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095702751.png&quot; alt=&quot;image-20220730095702751&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;프로파일링을 통해 병목지점을 파악하고,&lt;/li&gt;
  &lt;li&gt;특정 operation이 많은 시간을 차지한다는 것을 발견한다면, 해당 부분은 최적화해야 한다.&lt;/li&gt;
  &lt;li&gt;모델 자체를 최적화하여 더 빠르고 energy efficient한 모델을 만들수도 있며 특히 mobile 환경에서 중요하다.&lt;/li&gt;
  &lt;li&gt;사용하는 thread의 수를 늘리는 방법도 있다.
    &lt;ul&gt;
      &lt;li&gt;하지만 thread 수를 늘린다고 해서 무조건 성능이 좋아지는 것은 아니다.&lt;/li&gt;
      &lt;li&gt;무엇을 concurrent하게 실행하고 있는지 등에 따라 성능 차이가 발생한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;web-applications-for-users&quot;&gt;&lt;strong&gt;Web Applications for Users&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095709919.png&quot; alt=&quot;image-20220730095709919&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델에서 서비스를 받는 유저는 request를 web application을 통해한다.&lt;/li&gt;
  &lt;li&gt;Model은 API 서비스 형태로 존재하고, 이러한 과정을 돕는 다양한 web framework들이 존재한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;serving-systems-for-easy-deployment&quot;&gt;&lt;strong&gt;Serving systems for easy deployment&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095716511.png&quot; alt=&quot;image-20220730095716511&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델 서빙 시스템은 다양한 방식으로 모델 배포를 관리한다.
    &lt;ul&gt;
      &lt;li&gt;서버에 모델을 배포하여 서비스를 제공할 수 있으며,&lt;/li&gt;
      &lt;li&gt;Custom website가 필요하지 않고,&lt;/li&gt;
      &lt;li&gt;몇줄 안되는 코드로 배포가 가능하고,&lt;/li&gt;
      &lt;li&gt;Model update와 rollback을 편리하게 수행할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;clipper&quot;&gt;&lt;strong&gt;Clipper&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095723575.png&quot; alt=&quot;image-20220730095723575&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clipper는 UC berkely에서 만든 model serving 시스템이다.
    &lt;ul&gt;
      &lt;li&gt;다양한 모델 배포를 지원하며, restful API로 기존 application과 쉽게 통합할 수 있으며, 도커로 컨테이너화하여 자원 관리를 지원하며, latency setting을 할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving&quot;&gt;&lt;strong&gt;TensorFlow Serving&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095732320.png&quot; alt=&quot;image-20220730095732320&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving은 구글에서 만든 오픈소스이다.
    &lt;ul&gt;
      &lt;li&gt;텐서플로우 모델을 쉽게 배포할 수 있으며,&lt;/li&gt;
      &lt;li&gt;다른 타입의 모델도 배포할 수 있으며,&lt;/li&gt;
      &lt;li&gt;REST와 gRPC 프로토콜을 제공하며,&lt;/li&gt;
      &lt;li&gt;모델의 version management가 가능하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;advantages-of-serving-with-a-managed-service&quot;&gt;&lt;strong&gt;Advantages of Serving with a Managed Service&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095739374.png&quot; alt=&quot;image-20220730095739374&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google cloud와 같은 managed service를 사용하면 더 편할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;낮은 latency의 endpoint를 제공하고 대량의 batch를 처리할 수 있으며,&lt;/li&gt;
      &lt;li&gt;별도 환경 또는 클라우드에서 학습한 모델을 배포할 수 있으며,&lt;/li&gt;
      &lt;li&gt;traffic에 기반하여 자동으로 scaling을 수행하고&lt;/li&gt;
      &lt;li&gt;GPU/TPU와 같은 연산 가속기도 지원한다.
        &lt;ul&gt;
          &lt;li&gt;Google 외에도 MS, Amazon과 같은 기업들이 비슷한 서비스를 제공한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving을 위한 다양한 option들에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Introduction to Model Serving Infrastructure</title><link href="http://localhost:4000/lecture/2022/07/29/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving-Infrastructure/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Introduction to Model Serving Infrastructure" /><published>2022-07-29T00:00:00+09:00</published><updated>2022-07-29T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/29/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Introduction%20to%20Model%20Serving%20Infrastructure</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/29/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving-Infrastructure/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizing-models-for-serving&quot;&gt;&lt;strong&gt;Optimizing Models for Serving&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225850339.png&quot; alt=&quot;image-20220729225850339&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;최근 모델의 정확도를 높이기 위해 model에 feature 수를 늘리는 등 모델 자체의 complexity가 커지고 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;정확도는 높아지지만 이로인해 prediction latency도 커지고 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;as-model-complexity-increases-cost-increases&quot;&gt;&lt;strong&gt;As Model Complexity Increases Cost Increases&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225858482.png&quot; alt=&quot;image-20220729225858482&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델 complexity의 증가는 비용을 초래한다.&lt;/li&gt;
  &lt;li&gt;GPU, TPU와 같은 하드웨어와 model registry, maintenance에 있어서의 비용이 포함된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;balancing-cost-and-complexity&quot;&gt;&lt;strong&gt;Balancing Cost and Complexity&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225907449.png&quot; alt=&quot;image-20220729225907449&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델의 정확도와 prediction 속도 간에는 trade-off 관계가 존재하며, 이 사이의 균형을 맞추는 것이 중요하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizing-and-satisficing-metrics&quot;&gt;&lt;strong&gt;Optimizing and Satisficing Metrics&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225915162.png&quot; alt=&quot;image-20220729225915162&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모델의 정확도를 측정하는 metric으로 accuracy, precision, recall이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반면, 연산상의 제약 조건과 관련된 metric으로는 latency, model size, GPU load가 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizing-and-satisficing-metrics-1&quot;&gt;&lt;strong&gt;Optimizing and Satisficing Metrics&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225927745.png&quot; alt=&quot;image-20220729225927745&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이와 같은 metric을 만족하는 방법은 우선 특정한 serving infrastructure에서 model의 complexity를 증가시키는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Latency와 같은 연산 상의 제약조건 metric threshold에(e.g. latency) 걸리는 순간까지 정확도를 높인다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이후 최종 결과를 평가하고 난 후에 정확도를 높이거나, infra를 증축하거나, 모델의 complexity를 줄이거나와 같은 대책을 마련하고 실행한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-of-accelerators-in-serving-infrastructure&quot;&gt;&lt;strong&gt;Use of Accelerators in Serving Infrastructure&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225935034.png&quot; alt=&quot;image-20220729225935034&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Accelerator를 통해 infrastructure를 최적화할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GPU의 경우는 training 가속화에 강점을 보이며,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TPU는 large batch size, complex model inference에 강점이 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;maintaining-input-feature-lookup&quot;&gt;&lt;strong&gt;Maintaining Input Feature Lookup&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225942225.png&quot; alt=&quot;image-20220729225942225&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prediction을 수행할 때에는 많은 feature들이 필요할 수 있다.&lt;/li&gt;
  &lt;li&gt;예를들어 음식 배달 시간을 예측한다고 할때, 주문 수, 현재 교통 상황, 처리되지 않은 주문 수와 같은 많은 정보들이 필요하다.&lt;/li&gt;
  &lt;li&gt;Prediction latency를 줄이기 위해서는 사용되는 대량의 정보를 data store에서 빠르게 읽을 수 있어야 한다.&lt;/li&gt;
  &lt;li&gt;이때 cache를 사용하면 필요한 정보를 빠르게 읽어 latency를 낮출 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nosql-databases-caching-and-feature-lookup&quot;&gt;&lt;strong&gt;NoSQL Databases: Caching and Feature Lookup&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225950122.png&quot; alt=&quot;image-20220729225950122&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NoSQL은 caching과 feature look up을 구현하기에 좋으며, 위와 같이 다양한 옵션들이 존재한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving 인프라에서 고려해야할 점들에 대한 내용을 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Introduction to Model Serving</title><link href="http://localhost:4000/lecture/2022/07/28/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Introduction to Model Serving" /><published>2022-07-28T00:00:00+09:00</published><updated>2022-07-28T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/28/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Introduction%20to%20Model%20Serving</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/28/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-exactly-is-serving-a-model&quot;&gt;&lt;strong&gt;What exactly is Serving a Model?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/what%20exactly%20is%20Serving%20a%20Model.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;전체 ML 프로젝트에서 model training은 우리가 아는 굉장히 일부분이다.&lt;/li&gt;
  &lt;li&gt;그중 model serving은 학습시킨 model을 end user가 사용할 수 있도록 하는 것이며,&lt;/li&gt;
  &lt;li&gt;이를 위해서는 end user가 interaction할 수 있는 app 또는 서비스가 필요하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving-patterns&quot;&gt;&lt;strong&gt;Model Serving patterns&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Model%20Serving%20patterns.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inference는 모델에 입력 값을 넣고 예측을 하는 과정이며,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;크게 세가지 (model, interpreter, input data)가 필요하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-workflows&quot;&gt;&lt;strong&gt;ML workflows&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/ML%20workflows.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ML workflow는 크게 &lt;strong&gt;model training&lt;/strong&gt;, &lt;strong&gt;model prediction&lt;/strong&gt; 두 가지로 나뉜다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이때, model training은 &lt;strong&gt;offline learning(batch learning, static learning)&lt;/strong&gt;과 &lt;strong&gt;online learning(dynamic learning)&lt;/strong&gt;으로 나눌 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Offline learning&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Offline learning은 일정 기간 동안 이미 수집된 데이터를 바탕으로 특정 시기에 모델 학습을 시키는 것이다.&lt;/li&gt;
          &lt;li&gt;즉 기존 data와 새로 들어온 data를 함께 학습시키게 되며 대량의 데이터를 한번에 처리하기 때문에 학습시 많은 시간과 자원이 소요된다.&lt;/li&gt;
          &lt;li&gt;모델을 배포하면 재학습시까지 모델은 고정된 상태가 되며, 오랜 시간이 지나면 새로운 pattern에 대응하지 못하며 &lt;strong&gt;model decay&lt;/strong&gt;가 발생한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Online learning&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Online learning 은 실시간으로 스트림 데이터가 들어올 때 마다 주기적으로 모델을 학습시키는 것이다.&lt;/li&gt;
          &lt;li&gt;주로 sensor, stock 데이터와 같은 time-series data을 토대로 학습을 할때 많이 활용한다.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model prediction도 &lt;strong&gt;Batch prediction, Realtime prediction&lt;/strong&gt; 두 타입이 존재한다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Batch prediction&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Batch prediction은 과거에 모인 다량의 데이터를 기반으로 예측을 하는것이다.&lt;/li&gt;
          &lt;li&gt;즉 한번의 예측에서 많은 수의 인스턴스를 처리하게 된다.&lt;/li&gt;
          &lt;li&gt;데이터가 time dependent하지 않고, realtime으로 예측을 하는 것이 중요하지 않은 상황에 적절하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Realtime prediction&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Real time prediction은 inference 요청이 온 시점에 들어온 데이터를 기반으로 실시간으로 prediction을 하는 것이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;important-metrics&quot;&gt;&lt;strong&gt;Important metrics&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Important%20metrics.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Online inference에서 중요하게 살펴보는 metric으로는 latency, throughput, cost가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lantency&quot;&gt;&lt;strong&gt;Lantency&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Lantency.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Latency는 user의 요청과 이에대한 application의 응답까지의 시간을 의미한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;throughput&quot;&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Throughput.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Throughput은 단위 시간동안 처리한 요청의 수를 의미한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cost&quot;&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Cost.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Serving infrastructure는 반드시 cost를 수반하게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;minimizing-latency-maximizing-throughput&quot;&gt;&lt;strong&gt;Minimizing Latency, Maximizing Throughput&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Minimizing%20Latency,%20Maximizing%20Throughput.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;많은 기업들은 latency를 최소화하며, throughput을 최대화하길 원한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예를들어 항공사 추천 서비스가 있다고 할때, 유저의 요청에 빠르게 응답할 수 있어야하며, 공휴일과 같이 유저가 몰리는 시간에는 많은 요청을 빠르게 처리할 수 있어야 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 요건을 충족시키기 위해서는 infrastructure를 확장해야 하는데, 이때 큰 비용이 발생한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;balance-cost-latency-and-throughput&quot;&gt;&lt;strong&gt;Balance Cost, Latency and Throughput&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction%20to%20Model%20Serving/Balance%20Cost,%20Latency%20and%20Throughput.png&quot; alt=&quot;image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Infrastructure 확장에 많은 비용을 투자하지 않고 이러한 비용을 낮추기 위한 방법이 있다.
    &lt;ul&gt;
      &lt;li&gt;GPU 자원 공유&lt;/li&gt;
      &lt;li&gt;Multi model serving&lt;/li&gt;
      &lt;li&gt;Model optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving이란 무엇이며, 중요하게 고려해야할 metric에 대해 알아본다.</summary></entry><entry><title type="html">Multithreading vs. Multiprocessing</title><link href="http://localhost:4000/operating%20system/2021/09/19/Multiprocess&multithread/" rel="alternate" type="text/html" title="Multithreading vs. Multiprocessing" /><published>2021-09-19T00:00:00+09:00</published><updated>2021-09-19T00:00:00+09:00</updated><id>http://localhost:4000/operating%20system/2021/09/19/%20Multiprocess&amp;multithread</id><content type="html" xml:base="http://localhost:4000/operating%20system/2021/09/19/Multiprocess&amp;multithread/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;process&quot;&gt;&lt;strong&gt;Process&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;프로세스는 실행중인 프로그램이다.&lt;/strong&gt; 즉 프로그램 자체는 프로세스가 아니라, 프로그램이 메모리에 올라가서 실행중일 때 프로세스가 되는 것이다. 프로세스는 크게 5가지 영역(text, stack, data, heap, program counter)로 나눌 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Text : Program code&lt;/li&gt;
  &lt;li&gt;Stack : Temporary data (function parameter, return addresses, local variables, etc)&lt;/li&gt;
  &lt;li&gt;Data : Static &amp;amp; global variables&lt;/li&gt;
  &lt;li&gt;Heap : Dynamic allocation&lt;/li&gt;
  &lt;li&gt;Program counter &amp;amp; registers : Current activity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://www.tutorialspoint.com/operating_system/images/process_components.jpg&quot; alt=&quot;image&quot; width=&quot;45%&quot; height=&quot;45%&quot; class=&quot;align-center&quot; /&gt;
&lt;em&gt;Figure 1. Process in memory (&lt;a href=&quot;https://www.tutorialspoint.com/operating_system/os_processes.htm&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;thread&quot;&gt;&lt;strong&gt;Thread&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Thread는 프로세스 내에서 실행되는 단위를 말한다.&lt;/strong&gt; 일반적으로 한 프로세스는 하나의 스레드를 가지고 있지만, 프로그램 환경에 따라 둘 이상의 thread를 동시에 실행시킬 수 있다. 각 Thread는 program counter, register set, stack space를 별도로 지니고 있다. 반면, 각 thread는 프로세스 내 code, data, heap, file과 같은 operating system 자원을 공유하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://www.tutorialspoint.com/operating_system/images/thread_processes.jpg&quot; alt=&quot;image&quot; width=&quot;90%&quot; height=&quot;90%&quot; class=&quot;align-center&quot; /&gt;
&lt;em&gt;Figure 2. Single thread vs Multithreads (&lt;a href=&quot;https://www.tutorialspoint.com/operating_system/os_multi_threading.htm&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;multithreading-multiprocessing-비교&quot;&gt;&lt;strong&gt;Multithreading, multiprocessing 비교&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;multithreading--장점&quot;&gt;&lt;strong&gt;Multithreading : 장점&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Thread 간 Resource sharing이 가능하다.
    &lt;ul&gt;
      &lt;li&gt;동일 프로세스 내 thread 간 데이터를 공유할 수 있다. 만약 subset으로 나누기 어려운 대량의 데이터를 다뤄야 할 경우, 데이터를 복사하거나 shared memory 방식을 사용해서 프로세스 간 데이터를 전송 또는 공유해야 한다. 하지만 데이터 복사는 복사 시 추가 시간과 메모리를 소요하게 되며, shared memory의 경우에는 개발 시 고려해야 될 요소들이 많다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;소요되는 자원이 더 적다.
    &lt;ul&gt;
      &lt;li&gt;Thread를 하나 생성하는 것 보다 process를 생성할 때 소요되는 시간과 자원이 훨씬 더 많이 들어간다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multithreading--단점&quot;&gt;&lt;strong&gt;Multithreading : 단점&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;안정성이 떨어진다.
    &lt;ul&gt;
      &lt;li&gt;만약 여러 thread 들 중 단 하나의 thread라도 문제가 발생할 경우, application이 crash된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;디버깅 하기 어렵다.
    &lt;ul&gt;
      &lt;li&gt;Multithread 환경에서 디버거를 통해 버그의 원인을 발견하기 어렵다. 따라서 문제가 되는 thread를 발견하기 위해 보통 log를 찾아보며 추적하게 됨으로, 디버깅에 소요되는 시간이 많이 들어가게 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multiprocessing--장점&quot;&gt;&lt;strong&gt;Multiprocessing : 장점&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;안정성이 높다.
    &lt;ul&gt;
      &lt;li&gt;여러 프로세스 실행 중 하나의 프로세스에 문제가 발생할 경우, 해당 프로세스만 종료시키면 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;확장성이 크다.
    &lt;ul&gt;
      &lt;li&gt;프로세스는 다른 머신에 할당해서 실행할 수가 있다. 반면에 thread는 단일 프로세스 내에 존재해야 하므로 확장성이 떨어질 수밖에 없다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multiprocessing--단점&quot;&gt;&lt;strong&gt;Multiprocessing : 단점&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;프로세스 간 커뮤니케션이 복잡하다.
    &lt;ul&gt;
      &lt;li&gt;프로세스 간 데이터를 공유할 때는 별도의 technique (shared memory 또는 inter process communication)을 사용해야 하기 때문이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;메모리 소요가 크다.
    &lt;ul&gt;
      &lt;li&gt;프로세스 간에는 별도의 독립된 메모리 영역을 갖는다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Operating system" /><category term="operating system" /><category term="parallel programming" /><summary type="html">Multithreading과 multiprocessing 장단점에 대한 간략한 정리</summary></entry></feed>
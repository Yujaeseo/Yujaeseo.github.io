<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-14T18:47:50+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yu’s Tech 블로그</title><subtitle>You will never know until you try.</subtitle><author><name>Jaeseo Yu</name></author><entry><title type="html">Docker를 이용한 deep learning 개발 환경 구축 (2)</title><link href="http://localhost:4000/docker/2022/10/06/Docker-deep-learning-dev-environment2/" rel="alternate" type="text/html" title="Docker를 이용한 deep learning 개발 환경 구축 (2)" /><published>2022-10-06T00:00:00+09:00</published><updated>2022-10-06T00:00:00+09:00</updated><id>http://localhost:4000/docker/2022/10/06/Docker%20deep%20learning%20dev%20environment2</id><content type="html" xml:base="http://localhost:4000/docker/2022/10/06/Docker-deep-learning-dev-environment2/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;volume-생성&quot;&gt;&lt;strong&gt;Volume 생성&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;컨테이너 안의 데이터를 보존하기 위해서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volume&lt;/code&gt;을 생성해야 한다.  만약 volume을 생성하지 않고 컨테이너 안에서 텍스트 파일(e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test.txt&lt;/code&gt;)을 저장 후 실행 중인 컨테이너를 종료시켰다고 해보자. 이후 동일한 image를 사용하여 컨테이너를 재실행 했을 때, 해당  파일을 저장한 path로 가서 파일을 찾아보아도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test.txt&lt;/code&gt; 를 발견할 수 없다. 즉, 컨테이너에 mount한 volume 밖에 있는 data는 persistent한 성질을 갖지 않게 되고, 컨테이너 종료와 함께 사라지게 된다. 따라서 컨테이너의 lifecycle과 독립적으로 data를 저장하고 관리하기 위해서는 volume을 생성하고 해당 volume에 data를 저장해야한다.&lt;/p&gt;

&lt;p&gt;컨테이너 실행 시 아래의 command로 volume을 생성할 수 있다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--volume host path:container path&lt;/code&gt; 로 host의 directory (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./data&lt;/code&gt;) 와 container의 directory (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;home/jaeseo/app&lt;/code&gt;)을 매핑하였다. 이를 통해 컨테이너는 host 내의 디렉토리를 컨테이너의 디렉토리로 사용할 수 있게 된다. 이미 존재하는 volume과 매핑할 경우에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;host path&lt;/code&gt;에 존재하는 volume의 이름을 적으면 된다. 존재하지 않는 volume의 경우에는 자동으로 생성이 된다.  특정 volume을 여러 컨테이너가 공유할 수 있는데, 해당 host의 디렉토리를 다른 컨테이너에 동일하게 매핑하면 된다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   docker container run &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; development_env_cont &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; ./data:/home/jaeseo/app &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   development_env:1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;컨테이너를-외부와-연결&quot;&gt;&lt;strong&gt;컨테이너를 외부와 연결&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;컨테이너를 외부와 연결해주기 위하서는 도커와 호스트의 IP와 포트를 바인딩해야 한다. 만약 바인딩을 하지 않고서는 컨테이너를 외부에서 접근할 수 없으며, 컨테이너에 구성한 개발 환경도 호스트에서 사용할 수 없게된다. 그래서 호스트에서 컨테이너의 어플리케이션에 접근할 수 있도록 포트 바인딩을 한다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt; 옵션을 추가하고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;호스트 포트:컨테이너 어플리케이션 포트&lt;/code&gt; 의 형식으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; command에 추가하면 된다. 여러개의 포트를 개방하고싶다면, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt; 옵션을 여러 번 써서 설정한다. 필자는 jupyter notebook을 사용할 것이기 때문에, jupyter notebook이 기본적으로 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8888&lt;/code&gt; 포트를  호스트와 연결한다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   docker container run &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; development_env_cont &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; ./data:/home/jaeseo/app &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8888:8888 &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
   development_env:1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pytorch-jupyterlab-다운로드&quot;&gt;&lt;strong&gt;Pytorch, Jupyterlab 다운로드&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;머신러닝 프레임워크로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt;를 사용할 것이기 때문에, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;와 이를 사용하기 위한 연관 패키지를 다운받는다. 컴퓨터비전과 음성처리와 관련된 데이터, 모델 및 필요한 요소들을 포함하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio&lt;/code&gt;와 함께 다운로드한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt;를 통해 사전에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dockerfile&lt;/code&gt;에서 생성한 가상환경 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test&lt;/code&gt;)를 활성화하고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install&lt;/code&gt;을 해야한다는 점에 유의하자. 이후 다운로드가 완료되면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda list&lt;/code&gt;를 통해관련 package가 다운됐는지 확인해보자.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   conda activate &lt;span class=&quot;nb&quot;&gt;test
   &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pytorch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.12.0 &lt;span class=&quot;nv&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.13.0 &lt;span class=&quot;nv&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.12.0 &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; pytorch
   
   conda list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20221014100600904.png&quot; alt=&quot;image-20221014100600904&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt;가 정상적으로 다운로드된 것을 확인한 후에, 머신러닝 개발 환경에 특화된 웹 형태의 개발 환경으로 유명한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jupyterlab&lt;/code&gt;을 install한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jupyterlab&lt;/code&gt;은 기존 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jupyter notebook&lt;/code&gt;에 다중 탭 등 다양한 확장 기능을 추가해 만든 어플리케이션이다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; conda-forge jupyterlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;작업-container-image로-저장&quot;&gt;&lt;strong&gt;작업 container image로 저장&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;현재까지 처음에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dockerfile&lt;/code&gt;로 만들었던 이미지를 컨테이너로 생성하고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt;와 연관 패키지와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jupyterlab&lt;/code&gt;을 설치했다. 그리고 앞으로 머신러닝 및 딥러닝 모델 개발을 하면서 필요한 패키지가 그때그때 생길 것이고 설치하게 될것이다. 하지만, 이렇게 공들여 업데이트하고 만들어놓은 컨테이너는 컨테이너가 종료된 순간 모든 업데이트 정보가 사라지게 된다. 컨테이너에서 작업한 내용들은 컨테이너 상태로 저장되게 되고, 컨테이너를 생성한 이미지에 업데이트 되지 않기 때문이다. 다음날 다시 컨테이너를 생성하고 작업을 시작할 때마다 복잡한 설정을 일일이 해줄수도 없고 그렇다고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dockerfile&lt;/code&gt; 에 작업한 내용을 매번 다시 써서 파일을 업데이트하기도 귀찮다. Docker에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker commit&lt;/code&gt; 명령어를 통해 현재 특정 container를 이미지로 다시 저장할 수 있다. 저장된 이미지는 container에서 작업했던 내용을 포함하고 있고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commit&lt;/code&gt;한 이미지를 실행시켜 작업을 편리하게 이어갈 수 있다. 호스트에서  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker commit [컨테이너명] [이미지명]&lt;/code&gt;의 형식으로 쉽게 컨테이너를 저장할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   docker commit development_env_cont commit_pytorch_image
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker images&lt;/code&gt; 명령어를 통해 새로운 이미지가 생성된 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20221014162718322.png&quot; alt=&quot;image-20221014162718322&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;jupyterlab-실행&quot;&gt;&lt;strong&gt;Jupyterlab 실행&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;호스트에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab&lt;/code&gt; 비밀번호를 설정하고, notebook directory를 생성한다. 아래의 명령어를 통해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab&lt;/code&gt; 을 실행할 수 있다. 실행 포트는 이전 컨테이너 실행시에 매핑했던 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8888&lt;/code&gt;을 적는다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   jupyter lab password
   &lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;jupyterlab
   jupyter lab &lt;span class=&quot;nt&quot;&gt;--notebook-dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/app/jupyterlab &lt;span class=&quot;nt&quot;&gt;--ip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'*'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8888 &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--allow-root&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;호스트에서 브라우저 창을 열고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8888&lt;/code&gt;로 접속하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab&lt;/code&gt; 창이 뜨게되고, 설정한 password를 입력해 노트북을 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20221014164956399.png&quot; alt=&quot;image-20221014164956399&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Docker" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="docker" /><summary type="html">컨테이너 port, volume 설정 후 jupyterlab에서 pytorch를 실행시켜본다.</summary></entry><entry><title type="html">Docker를 이용한 deep learning 개발 환경 구축 (1)</title><link href="http://localhost:4000/docker/2022/09/04/Docker-deep-learning-dev-environment/" rel="alternate" type="text/html" title="Docker를 이용한 deep learning 개발 환경 구축 (1)" /><published>2022-09-04T00:00:00+09:00</published><updated>2022-09-04T00:00:00+09:00</updated><id>http://localhost:4000/docker/2022/09/04/Docker%20deep%20learning%20dev%20environment</id><content type="html" xml:base="http://localhost:4000/docker/2022/09/04/Docker-deep-learning-dev-environment/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;개요&quot;&gt;&lt;strong&gt;개요&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;이번 글은 딥러닝을 위한 개발 환경을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker&lt;/code&gt;를 이용해 구축하는 일부 과정을 담았다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ubuntu&lt;/code&gt;에 딥러닝 개발에 많이 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anaconda&lt;/code&gt; 프레임워크를 포함하는 개발 환경을 구축하기 위한 dockerfile을 작성하고, 이를 토대로 docker image build, container 실행까지 해본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dockerfile-작성&quot;&gt;&lt;strong&gt;Dockerfile 작성&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dockerfile&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;은  docker image를 만들기 위한 command들이 모여있는 text 파일이다. Dockerfile의 파일 이름은 별도의 확장자 없이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;이라고 설정하는 것이 권장된다. 따라서, 프로젝트 디렉토리에 파일을 생성한 후 아래의 내용을 참고해서 dockerfile을 작성하면된다. 파일을 작성한 후에는 해당 파일을 build하여 image를 생성하고, 이를 실행하면 container를 생성한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Base image 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ubuntu:18.04&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Image를 만들때는 작업의 효율성을 위해 보통 다른 사람들이 많들어 놓은 image를 기반으로 그 위에 하나씩 layer를 쌓게된다. 만약 이미지의 모든 부분을 스스로 만든다고 하면 모든 dependecy 및 setting을 고려하여 작업을 해야하기 때문에 이미지 제작에 소요되는 시간이 무척 오래 걸릴 것이다.  AI를 전공하는 사람들이 가장 많이 사용하는 linux 운영체제 중 하나가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt;이기 때문에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; 이미지를 base image로 설정한다. 이때 Base image는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; 명령어를 통해 설정할 수 있다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; 뒤에 원하는 이미지와 태그명을 쓰면 된다. 이후에 base image 위에 anaconda를 구동하기 위한 환경 설정을 하고 anaconda를 install한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Shell 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SHELL&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; , &quot;-o&quot;, &quot;pipefail&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Image build 시에 사용할 shell을 변경하려면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHELL&lt;/code&gt; 명령어를 사용해야 한다. 사용 형태는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHELL [&quot;executable&quot;, &quot;parameters&quot;]&lt;/code&gt;으로 해당 명령어는 기존에 설정된 shell을 override 할수 있게 해준다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHELL&lt;/code&gt; 명령어는 파일 내에서 여러번 나올 수 있는데, 이때는 나중에 나온 설정이 이전 설정을 override하게 된다.&lt;/p&gt;

    &lt;p&gt;Linux의 default shell은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/sh&lt;/code&gt;이지만, 현재 anaconda는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/sh&lt;/code&gt;를 지원하지 않으므로, ubuntu에서 일반적으로 많이 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash&lt;/code&gt;를 default로 세팅해준다. 이때  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--login&lt;/code&gt; 옵션으로  login shell을 실행하도록 설정한다.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-c&lt;/code&gt;  인자는 string에서 command를 읽도록 설정하는 옵션이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-o pipefail&lt;/code&gt; 옵션은 이후에 user 설정에서 실행할 pipeline을 위해 넣어준 옵션으로서, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt;를 이용해서 연결된 command를 실행할 때 도중에 error가 발생하면 이를 시스템과 사용자가 발견할 수 있도록해준다. 해당 설정을 사용하지 않으면, pipeline 실행의 return 값이 마지막에 실행된 command에서 나오게 된다. 따라서 마지막 command만 정상적으로 수행된다면 이전에 발생한 error를 발견할 수 없다. 반면에 해당 옵션을 사용하면, return 값이 오른쪽으로 전달되어 가장 오른쪽에서 발생한 error가 pipeline 실행 결과 값으로 나오게되고 해당 return 값을 통해 오류 발생 여부를 판단할 수 있게된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;apt-get 업데이트 및 wget 설치&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;RUN  &lt;/span&gt;apt-get update &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; wget &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /var/lib/apt/lists/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Anaconda installer를 서버에 설치해야 되는데, 이때 ubuntu에서 웹서버에서 프로그램을 다운로드 할때  주로 사용되고 있는&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wget&lt;/code&gt;를 이용하려고 한다. 이를 위해서 패키지 관리 툴인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt(Advanced Package Tool)&lt;/code&gt;가 install, upgrade, cleaning를 위해 제공하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;을 업데이트 한 후에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wget&lt;/code&gt;을 install 한다. 필요한 패키지를 설치한 후에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt&lt;/code&gt;가 관리하는 패키지에 대한 정보들이 저장되어 있는 저장소(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/apt/lists&lt;/code&gt;)를 비워 이미지 사이즈를 줄인다.  추가적으로 프로그램 실행 권한을 관리하는 명령어인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt;도 설치한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Non-root user 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;기본적으로 docker container는 root 권한을 갖고 실행되도록 세팅이 되어있다. 이때 container 안에서 실행되는 application들도 root 권한을 갖게 된다. 이때 만약 해커가 호스트에서 실행되는 container 및 application을 해킹하면 호스트 시스템의 root 권한을 얻어 여러 resource에 접근할 수 있게 되어 보안상 큰 문제가 발생할 수 있게된다. 따라서, 일반적으로 container를 non-root user로 실행하여 발생할 수 있는 보안상의 문제를 완화할 필요가 있다.&lt;/p&gt;

    &lt;p&gt;하지만, 현재 생성하고자 하는 docker container는 딥러닝 개발을 위해 개인적으로 사용하려고 하는 목적이기 때문에 user를 생성하되, sudo 권한을 부여하여 컨테이너 설정 및 이용을 편하게 할수 있도록 한다.&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; username=ubuntu&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uid=11&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; upwd=123&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; USER=$username UID=$uid UPWD=$upwd HOME=/home/$username&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;useradd &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-G&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /bin/bash &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$UID&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$UPWD&lt;/span&gt; | chpasswd
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'%sudo ALL=(ALL) NOPASSWD:ALL'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/sudoers
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $HOME&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt;는 docker image build시에 사용할 수 있는 값들을 설정할 때 쓰이는 값이다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENV&lt;/code&gt;와의 차이는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt;에서 설정한 값은 image build 시점에서만 사용되지만, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENV&lt;/code&gt;에서 설정한 값은 image build 시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt;command와 build 후 container runtime에서 사용될 수 있다는 점이다. 두 명령어를 통해 user 정보와 관련된 값들을 설정하고, 이 값들을 유저를 추가할 수 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;useradd&lt;/code&gt;  명령어와 함께 사용하여 sudo 권한을 갖는 유저를 생성한다. 이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/sudoers&lt;/code&gt;에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'%sudo ALL=(ALL) NOPASSWD:ALL'&lt;/code&gt;를 추가하여 계정 password를 치지않고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt; 명령어를 사용할 수 있게 설정한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Anaconda 설치&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;USER&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $USER&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; URL_PREFIX=https://repo.anaconda.com/miniconda \&lt;/span&gt;
	INSTALLER_URL=$URL_PREFIX/Miniconda3-latest-Linux-x86_64.sh \
	CONDA_DIR=~/miniconda3
     	
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;wget &lt;span class=&quot;nt&quot;&gt;--quiet&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$INSTALLER_URL&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/miniconda.sh &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	&lt;span class=&quot;nb&quot;&gt;chmod &lt;/span&gt;u+x ~/miniconda.sh &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	~/miniconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; ~/miniconda.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USER $USER&lt;/code&gt;를 통해 root가 아닌 생성한 user로 이미지 생성 및 컨테이너를 사용할수 있도록 세팅한다. 이후 전 과정에서 설치한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wget&lt;/code&gt;으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;miniconda&lt;/code&gt;를 설치한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;miniconda&lt;/code&gt;는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anaconda&lt;/code&gt;와 다르게 최소한의 패키지만을 포함하는 압축된 버전이다. 기본적인 요구 사항들만을 포함하고 있기 때문에 원하는 패키지를 그때그때 다운받아야하지만, 가볍고 빠르다는 장점이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conda 기본 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; PATH=$CONDA_DIR/bin:$PATH&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;conda init bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Bash에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda&lt;/code&gt; 명령어 사용을 위한 환경 변수 설정과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda init bash&lt;/code&gt; 를 통해 bash에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt;  명령어를 비롯하여 conda에서 제공하는 다양한 기능을 사용할 수 있도록 설정해준다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Project directory 설정&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; PROJECT_DIR ~/app&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PROJECT_DIR&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $PROJECT_DIR&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;진행하고자하는 프로젝트에 필요한 소스코드, 소프트웨어를 비롯한 파일을 모아둘 폴더(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app&lt;/code&gt;)를 home directory(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/&lt;/code&gt;)에 생성한다. 프로젝트 별로 필요한 여러 파일들을 하나의 directory에 저장하고 관리하면 깔끔하고 관리의 용이성도 커진다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conda environment 생성&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Conda environment를 이전 과정에서 만든 directory에 생성한다. 이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda create&lt;/code&gt;명령어를 사용하며 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt;  인자를 통해 가상환경을 생성할 폴더의 path(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ENV_PREFIX/test&lt;/code&gt;)를 지정한다. 가상환경이 생성은 됐지만, 가상환경의 이름은 지정되지 않은 상황이다. 따라서 가상환경을 지칭할 때 full path를 입력해야된다. 하지만 가상환경을 활성화하는 것을 비롯하여 가상환경을 이용할 때 이를 지칭하는 이름이 있어야 명령어가 짧아지고 관리하기 편해진다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda config --append&lt;/code&gt; 명령어에 가상환경 폴더(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ENV_PREFIX/test&lt;/code&gt;)의 parent directory (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ENV_PREFIX&lt;/code&gt;)를 넘겨주면, conda configuration 파일인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.condarc&lt;/code&gt; 에 가상환경 directory를 추가할 수 있으며, 이를 통해가상환경을 지칭하는 이름을 설정할 수 있다.&lt;/p&gt;

    &lt;p&gt;가상환경 생성, 이름 세팅을 별도의 명령어로 나누어서 진행했지만, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda create&lt;/code&gt; 에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-n&lt;/code&gt; 인자를 통해 가상환경 생성시에 이름을 설정할 수 있다. 다만, 구체적인 이유는 모르겠지만 가상환경 폴더의 path를 설정하는 인자(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt;)와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-n&lt;/code&gt;를 함께 쓸 수 없기 때문에 차선책으로 가상환경을 지정된 위치에 생성, 이름을 지정하는 두번의 step으로 나누어서 진행하였다.&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ENV_PREFIX $PROJECT_DIR/env&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;conda update &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; base &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; defaults conda &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	conda create &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;/test &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	conda config &lt;span class=&quot;nt&quot;&gt;--append&lt;/span&gt; envs_dirs &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;	conda clean &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--yes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Container 실행 명령 정의&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/bin/bash&quot;, &quot;-l&quot;, &quot;-i&quot;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;USER&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $USER&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Entrypoint&lt;/code&gt; 명령어를 이용하여 컨테이너 시작시 실행할 명령을 지정할 수 있다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash&lt;/code&gt;를 login shell로 실행하도록 설정한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;image-build-및-container-실행&quot;&gt;&lt;strong&gt;Image build 및 container 실행&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Build image&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-powershell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;docker&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$UID&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;upwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$UPWD&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_TAG&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;작성한 Dockerfile을 이용하여 image를 생성하기 위해서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt; command를 사용한다. Dockerfile이 존재하는 디렉토리 경로를 넘겨주게 되면, 자동으로 Dockerfile을 읽어 image를 생성한다.이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tag&lt;/code&gt; 옵션을 통해 이미지 이름과 tag 정보를 추가할 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--build-arg&lt;/code&gt; 를 이용하면 build 시에 Dockerfile 내의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt;의 대응되는 key 값에 사용자가 넣은 값을 할당할 수 있게된다.  계정 패스워드와 같은 민감한 정보를 이와 같은 식으로 넘겨주는 것은 피해야하며 docker에서 제공하는  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secret&lt;/code&gt;과 같은 기능을 사용하여 정보를 넘겨주는 것이 좋다. 하지만 이번 프로젝트의 목적은 개인을 위한 딥러닝 개발 환경을 시험삼아 한번 만들어 보는 것이기 때문에 이와 같은 사항을 고려하지 않기로 한다. 참고로 필자는 window powershell에서 프로젝트를 진행하기 때문에, backslash(\)가 아닌 backtick(`)을 사용하여 command의 행을 나눴다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Run container&lt;/strong&gt;&lt;/p&gt;

    &lt;div class=&quot;language-powershell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;docker&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMAGE_TAG&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; 명령어와 이미지 이름 및 태그 정보를 통해 생성한 docker 이미지를 container로 만들 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-it&lt;/code&gt; 옵션은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--interactive&lt;/code&gt; ,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tty&lt;/code&gt;가 결합된 것으로, 컨테이너가 바로 종료되지 않고 현재 작업하고 있는 terminal에서 CLI를 통해 컨테이너를 조작할 수 있도록 만들어준다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;결과&quot;&gt;&lt;strong&gt;결과&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;container를 실행하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anaconda&lt;/code&gt;가 설치된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; 환경에 접속할 수 있게 되고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda env list&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt;를 통해 원하는 가상환경이 제대로 설치됐는지 확인하고 실행해볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220906162127088.png&quot; alt=&quot;image-20220906162127088&quot; /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Docker" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="docker" /><summary type="html">Ubuntu, anaconda 기반 딥러닝 개발 환경을 docker를 이용해서 직접 구축해본다. 이미지 생성을 위한 Dockerfile 작성에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Servers-Model Servers_Other Providers</title><link href="http://localhost:4000/lecture/2022/08/09/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Model-Servers_Other-Providers/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Servers-Model Servers_Other Providers" /><published>2022-08-09T00:00:00+09:00</published><updated>2022-08-09T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/08/09/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Model%20Servers-Model%20Servers_Other%20Providers</id><content type="html" xml:base="http://localhost:4000/lecture/2022/08/09/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Model-Servers_Other-Providers/">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;NVIDIA Triton Inference Server&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202430432.png&quot; alt=&quot;image-20220808202430432&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Triton Inference Server는 NVIDIA에서 제공하는 model server이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;오픈소스 inference 서빙 플랫폼으로서 tensorflow, pytorch, ONNX 등 타 딥러닝 프레임워크에서 학습된 모델들도 deploy를 할수 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Architecture&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202555416.png&quot; alt=&quot;image-20220808202555416&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;서로 다른 framework에서 만들어진 모델을 single GPU에서 concurrently하게 실행할 수 있다.&lt;/li&gt;
      &lt;li&gt;Multi GPU 환경에서는 각 모델에 대한 instance가 생성되며 각 GPU에 할당된다.
        &lt;ul&gt;
          &lt;li&gt;User의 별도 coding 없이 GPU utilization을 높일 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch inference와 streaming input에 대한 inference도 지원한다.&lt;/li&gt;
      &lt;li&gt;Performance를 높이기 위해 inference input, output을 CUDA GPU shared memory에 적재할 수도 있다.
        &lt;ul&gt;
          &lt;li&gt;httpc, grpc overhead를 줄일 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Designed for Scalability&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202603559.png&quot; alt=&quot;image-20220808202603559&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Autoscaling, orchestration을 위해 K8s와 결합되었고, end-to-end ML workflow를 위해 Kubeflow와도 결합된다.&lt;/li&gt;
      &lt;li&gt;Http, grpc API를 지원하며 이를 통해 load balancer 통신할 수 있다.&lt;/li&gt;
      &lt;li&gt;수많은 모델들이 동시에 serving될 수 있으며, GPU 뿐만 아니라 model을 heterogeneous한 환경에서 돌릴 수 있다.
        &lt;ul&gt;
          &lt;li&gt;Peak load 시에는 여러 프로세서 자원에 load를 분산시켜 효율적으로 대응할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Torch Serve&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202611455.png&quot; alt=&quot;image-20220808202611455&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Torchserve는 AWS와 Facebook이 함께 만들었으며, Pytorch model을 위한 serving framework이다.&lt;/li&gt;
      &lt;li&gt;여러 기능을 제공하며, 무엇보다 opensource여서 자신에게 fit하게 customize할 수 있다는 장점이 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (1)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202619768.png&quot; alt=&quot;image-20220808202619768&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Torchserve의 아키텍쳐는 다음과 같습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (2)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202628376.png&quot; alt=&quot;image-20220808202628376&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Frontend는 request와 response 처리와 model lifecycle을 관리한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (3)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202636889.png&quot; alt=&quot;image-20220808202636889&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Backend는  model store로 부터 load된 model instance를 실행하며, inference를 담당한다.&lt;/li&gt;
      &lt;li&gt;여러 model worker가 실행중인 것을 볼 수 있으며, 동일 model에 대한 여러 worker , 또는 서로 다른 model에 대한 여러 worker가 실행될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (4)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202645785.png&quot; alt=&quot;image-20220808202645785&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;모델은 cloud 또는 local storage로 부터 읽을 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TorchServe Architecture (5)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202654281.png&quot; alt=&quot;image-20220808202654281&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Server는 management, inference를 위한 API 또한 제공한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KFServing&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220808202703639.png&quot; alt=&quot;image-20220808202703639&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Kubeflow 또난 KF serving을 통한 serving 서비스를 제공하며, 여러 framework에서 만들어진 모델 serving을 지원한다.&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Tensorflow Serving외에 Triton Inference Server 등 다른 server에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Servers-Model Servers_Other Providers</title><link href="http://localhost:4000/lecture/2022/08/09/Deploying-Machine-Learning-Models-in-Production-Scaling-Infrastructure/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Servers-Model Servers_Other Providers" /><published>2022-08-09T00:00:00+09:00</published><updated>2022-08-09T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/08/09/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Scaling%20Infrastructure</id><content type="html" xml:base="http://localhost:4000/lecture/2022/08/09/Deploying-Machine-Learning-Models-in-Production-Scaling-Infrastructure/">&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Why is Scaling Important&lt;/strong&gt;		&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809161933007.png&quot; alt=&quot;image-20220809161933007&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;​     &lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809161953189.png&quot; alt=&quot;image-20220809161953189&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162006307.png&quot; alt=&quot;image-20220809162006307&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162018137.png&quot; alt=&quot;image-20220809162018137&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162028979.png&quot; alt=&quot;image-20220809162028979&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162039881.png&quot; alt=&quot;image-20220809162039881&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162051590.png&quot; alt=&quot;image-20220809162051590&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162105711.png&quot; alt=&quot;image-20220809162105711&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162121276.png&quot; alt=&quot;image-20220809162121276&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162149185.png&quot; alt=&quot;image-20220809162149185&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162159768.png&quot; alt=&quot;image-20220809162159768&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162209597.png&quot; alt=&quot;image-20220809162209597&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162219697.png&quot; alt=&quot;image-20220809162219697&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162229719.png&quot; alt=&quot;image-20220809162229719&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162240937.png&quot; alt=&quot;image-20220809162240937&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162254767.png&quot; alt=&quot;image-20220809162254767&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162304208.png&quot; alt=&quot;image-20220809162304208&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162323215.png&quot; alt=&quot;image-20220809162323215&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162617491.png&quot; alt=&quot;image-20220809162617491&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162632268.png&quot; alt=&quot;image-20220809162632268&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162653274.png&quot; alt=&quot;image-20220809162653274&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162707899.png&quot; alt=&quot;image-20220809162707899&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162720536.png&quot; alt=&quot;image-20220809162720536&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220809162742775.png&quot; alt=&quot;image-20220809162742775&quot; /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Tensorflow Serving외에 Triton Inference Server 등 다른 server에 대해 알아본다.</summary></entry><entry><title type="html">NVIDIA GPUDirect storage에 대해</title><link href="http://localhost:4000/hpc/2022/08/07/NVIDIA-GPUDirect-storage/" rel="alternate" type="text/html" title="NVIDIA GPUDirect storage에 대해" /><published>2022-08-07T00:00:00+09:00</published><updated>2022-08-07T00:00:00+09:00</updated><id>http://localhost:4000/hpc/2022/08/07/NVIDIA%20GPUDirect%20storage</id><content type="html" xml:base="http://localhost:4000/hpc/2022/08/07/NVIDIA-GPUDirect-storage/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-gpudirect&quot;&gt;&lt;strong&gt;NVIDIA GPUDirect&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA GPUDirect란 NVIDIA data center GPU의 data movement와 access 속도를 높여주는 기술이다. AI를 위해 많은 기업들이 고성능 GPU 서버를 구축하고 있고, 높은 flops를 가지는 GPU의 성능을 활용하기 위해서는 높은 IO bandwidth가 뒷받침되어야 한다. 아래의 이미지는 NVIDIA 기술 세미나에서 인용한 HP의 슬라이드이다. Computer architecture 각 구성 요소들의 처리 속도를 사람의 걸음수로 환산하여 비교한 자료이다. CPU 프로세서의 처리 속도를 한 걸음이라고 했을 때, system memory (DRAM)와 SSD (Flash)의 IO 속도를 각각 100, 200,000 걸음으로 환산할 수 있다. CPU 처리 속도에 비해 주변 장치의 IO 속도가 훨씬 느리기 때문에 processor의 빠른 처리 속도를 십분 활용하기 위해서 IO 속도를 높이는 것이 무엇보다 중요하다고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220807223108967.png&quot; alt=&quot;image-20220807223108967&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 1. Computer architecture (&lt;a href=&quot;https://www.youtube.com/watch?v=ZMf64oB_arY&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NIVIDIA GPU의 빠른 처리 속도를 활용해 시스템 성능을 극대화하기 위해 GPUDirect라는 기술을 개발하였고, storage access 최적화를 위한 GPUDirect Storage, network adapter access 최적화를 위한 GPUdirect RDMA, GPU 간 통신 최적화를 위한 GPUDirect P2P를 비롯한 다양한 기술들이 포함되어 있다. 그중 스토리지와의 data movement 속도를 높여주는 NVIDIA GPUDirect Storage(GDS)에 대해 알아보고자 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-gpudirect-storage-gds&quot;&gt;&lt;strong&gt;NVIDIA GPUDirect storage (GDS)&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GDS는 local 또는 remote storage(e.g. NVMe or NVMe over Fabric)와 GPU 간 direct path를 이용하여 빠르게 통신할 수 있도록 해주는 기술이다. 이를 통해 IO 시 CPU에서 발생하는 bottleneck을 줄여줄 수 있게된다. GDS가 없을 때는 local 또는 remote storage에서 데이터를 불러올 때 CPU를 거쳐야만 했다. 즉 CPU의 low memory에 있는 bounce buffer에 storage에서 읽은 데이터가 쌓이고, 해당 데이터를 high memory에 copy한 후 해당 데이터가 GPU 메모리로 이동하는 방식이였다. 반면에 GDS를 사용하면 아래의 그림과 같이 CPU를 거치지 않고 storage와 GPU간에 direct하게 통신할 수 있게 된다. GPU와 storage 간의 DMA (Direct Memory Access)를 통해 CPU의 불필요한 memory copy를 줄일 수 있게 되며, 통신을 위해 거치는 data path가 짧아져 data transfer에 소요되는 시간이 줄게된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/GPUDirect Storage data path.png&quot; alt=&quot;GPUDirect Storage data path&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 2. GPUDirect Storage data path (&lt;a href=&quot;https://nvdam.widen.net/s/k8vrp9xkft/tech-overview-magnum-io-1790750-r5-web&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;아래의 그래프는 NVIDIA 자체 gdsio benchmarking tool을 이용해서 bandwidth와 cpu utilization을 측정한 자료이다. 자료에 따르면 GDS를 사용하지 않았을 때(CPU-GPU_READ)에 비해 bandwidth는 최대 1.5X 향상되었으며, CPU utilization의 경우 최대 2.8X 향상되었음을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/The benefits of using GDS with the gdsio benchmarking tool.png&quot; alt=&quot;The benefits of using GDS with the gdsio benchmarking tool&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 3. The benefits of using GDS with the gdsio benchmarking tool (&lt;a href=&quot;https://nvdam.widen.net/s/k8vrp9xkft/tech-overview-magnum-io-1790750-r5-web&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Weather pattern을 확인하는 climate simulation deep learning 모델에 GDS, DALI (Nvidia data loading library), deepCAM를 적용하여 inference 성능을 측정했을 때 최대 6.6X의 성능 향상이 있었다. (GDS만 적용했을 때의 성능 개선치만 볼 수 있었다면 좋았을 것 같다).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/Performance benchmarking done for DeepCAM Inference.png&quot; alt=&quot;Performance benchmarking done for DeepCAM Inference&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;
  &lt;em&gt;Fig 4. Performance benchmarking done for DeepCAM Inference using standard GDS configuration in DGX A100 (&lt;a href=&quot;https://nvdam.widen.net/s/k8vrp9xkft/tech-overview-magnum-io-1790750-r5-web&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="HPC" /><category term="nvidia" /><category term="gpu" /><category term="hpc" /><category term="deep learning" /><category term="machine learning" /><summary type="html">GPU와 storage 간 IO 퍼포먼스를 향상시키는 NVIDIA GPUDirect Storage에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Servers-TensorFlow Serving</title><link href="http://localhost:4000/lecture/2022/08/02/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Tensorflow-Serving/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Servers-TensorFlow Serving" /><published>2022-08-02T00:00:00+09:00</published><updated>2022-08-02T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/08/02/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Model%20Servers-Tensorflow%20Serving</id><content type="html" xml:base="http://localhost:4000/lecture/2022/08/02/Deploying-Machine-Learning-Models-in-Production-Model-Servers-Tensorflow-Serving/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving1&quot;&gt;&lt;strong&gt;Tensorflow Serving(1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230627297.png&quot; alt=&quot;image-20220801230627297&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving은 tensorflow model을 바로 서빙할 수 있으며, Non TF model도 serving이 가능하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving2&quot;&gt;&lt;strong&gt;Tensorflow Serving(2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230635154.png&quot; alt=&quot;image-20220801230635154&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다량의 request를 동시에 처리하는 batch inference 기능을 제공한다.
    &lt;ul&gt;
      &lt;li&gt;Recommendation engine에서 주로 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;단일 request에 대해 빠른 inference 결과를 제공하는 real-time inference 기능도 제공한다.
    &lt;ul&gt;
      &lt;li&gt;Image classification task에서 주로 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving3&quot;&gt;&lt;strong&gt;Tensorflow Serving(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230643875.png&quot; alt=&quot;image-20220801230643875&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 task에대해 여러 모델을 serving하는 것도 가능하다.
    &lt;ul&gt;
      &lt;li&gt;A/B test에 유용하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving4&quot;&gt;&lt;strong&gt;Tensorflow Serving(4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230657706.png&quot; alt=&quot;image-20220801230657706&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rest endpoint도 제공한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture1&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230706249.png&quot; alt=&quot;image-20220801230706249&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High level architecture는 이와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture2&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230714137.png&quot; alt=&quot;image-20220801230714137&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TF serving은 TF servable을 중심으로 만들어졌으며, TF servable은 추상화된 객체로서 client가 inference나 lookup과 같은 연산을 수행할 때 사용한다.&lt;/li&gt;
  &lt;li&gt;전형적인 servable은 model이나 lookup table이 될수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture3&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230722617.png&quot; alt=&quot;image-20220801230722617&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loader로 TF servable의 lifecycle을 관리한다.&lt;/li&gt;
  &lt;li&gt;Loader API는 일반적인 인프라와 머신러닝 알고리즘, 데이터를 비롯한 태스크와 independent하게 만들어 준다.&lt;/li&gt;
  &lt;li&gt;Loader API로 servable을 load, unload 할수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture4&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230729609.png&quot; alt=&quot;image-20220801230729609&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source는 loader를 통해 여러 set으로 이루어진 서로 다른 version의 servable이 manager에게 전달한다.&lt;/li&gt;
  &lt;li&gt;새로운 servable이 전달되면 기존의 것은 unload한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture5&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(5)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230736712.png&quot; alt=&quot;image-20220801230736712&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이처럼 manager는 servable의 lifecycle을 통제하며, servable을 loading하거나 list에 없는 servable을 unload할 수 있다.&lt;/li&gt;
  &lt;li&gt;새로운 stream이 들어오면 version policy에 따라 load, unload한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture6&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(6)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230744753.png&quot; alt=&quot;image-20220801230744753&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Servable handler는 client가 servable을 이용할 수 있도록 interface를 제공한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture7&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(7)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230752000.png&quot; alt=&quot;image-20220801230752000&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;전반적인 프로세스를 이해하기 위해 예시를 들어보자.&lt;/li&gt;
  &lt;li&gt;Source가 빈번하게 weight이 update되는 tensorflow graph라고 가정하자.&lt;/li&gt;
  &lt;li&gt;이러한 weight은 filesystem에 저장이된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture8&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(8)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230759184.png&quot; alt=&quot;image-20220801230759184&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model의 weight가 바뀌었다고 할때, source는 새로운 버전의 모델을 감지한다.&lt;/li&gt;
  &lt;li&gt;디스크에 있는 모델 데이터의 pointer를 담은 loader를 만든다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture9&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(9)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230805817.png&quot; alt=&quot;image-20220801230805817&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source는 dynamic manager한테 이러한 version을 전달한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture10&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(10)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230818129.png&quot; alt=&quot;image-20220801230818129&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic manger는 version policy에 따라 새로운 version을 load한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture11&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(11)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230830401.png&quot; alt=&quot;image-20220801230830401&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;메모리가 충분하면, loader는 manager의 요청에의해 변경된 weight이 반영된 tensorflow graph를 servable로 instance화한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving-architecture12&quot;&gt;&lt;strong&gt;TensorFlow Serving Architecture(12)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220801230838113.png&quot; alt=&quot;image-20220801230838113&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Client는 새로운 버전에 대한 handle을 요청하면 manager는 servable에 대한 handle을 return한다.&lt;/li&gt;
  &lt;li&gt;Client는 해당 servable을 이용하여 inference를 수행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-참고자료&quot;&gt;&lt;strong&gt;# 참고자료&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;아키텍처 전반과 용어에 대한 설명이 적혀있는 참고자료
    &lt;ul&gt;
      &lt;li&gt;https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/architecture.md&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model server 중 하나인 tensorflow serving에 대한 아키텍처를 살펴본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Installing TensorFlow Serving</title><link href="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Installing-tensorflow-serving/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Installing TensorFlow Serving" /><published>2022-07-31T00:00:00+09:00</published><updated>2022-07-31T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/31/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Installing%20tensorflow%20serving</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Installing-tensorflow-serving/">&lt;hr /&gt;

&lt;h2 id=&quot;install-tensorflow-serving-1&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110109225.png&quot; alt=&quot;image-20220731110109225&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving을 설치하는 가장 쉬운 방법은 docker image를 사용하는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-tensorflow-serving-2&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110124591.png&quot; alt=&quot;image-20220731110124591&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Binaries를 이용해서 직접 다운로드 받는 방법은 다음과 같다.&lt;/li&gt;
  &lt;li&gt;Tensorflow-model-server는 플랫폼에 종속된 optimization 방식이 적용된 반면,&lt;/li&gt;
  &lt;li&gt;Tensorflow-model-server-universal은 플랫폼에 종속된 optimization 방식이 적용되지 않았다는 점이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-tensorflow-serving-3&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110132863.png&quot; alt=&quot;image-20220731110132863&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;직접 customize를 하길 원한다면 source로도 build를 할 수 있다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;install-tensorflow-serving-4&quot;&gt;&lt;strong&gt;Install Tensorflow Serving (4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110141743.png&quot; alt=&quot;image-20220731110141743&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving을 다운로드 받을 수 있는 archive 위치 정보를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/apt/sources.list.d&lt;/code&gt;에 등록한다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;인증을 위한 key를 불러오고, apt 업데이트 후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;을 이용하여 tensorflow serving을 다운받는다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;import-the-mnist-dataset&quot;&gt;&lt;strong&gt;Import the MNIST Dataset&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110150647.png&quot; alt=&quot;image-20220731110150647&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MNIST 데이터셋을 이용하여 비전 모델을 학습시킨다.
    &lt;ul&gt;
      &lt;li&gt;70,000장 (train 60,000/ test 10,000)의 0-9 grayscale 이미지(28x28)로 이루어져있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Numpy array 형식으로 데이터를 불러오고 0-1 scale 값으로 normalization을 수행한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;import-the-mnist-dataset-1&quot;&gt;&lt;strong&gt;Import the MNIST Dataset&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110209791.png&quot; alt=&quot;image-20220731110209791&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train, test 각각을 (60,000 x 28 x 28 x 1), (10,000 x 28 x 28 x 1) 형식의 array로 변환한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;look-at-a-sample-image&quot;&gt;&lt;strong&gt;Look at a Sample Image&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110217800.png&quot; alt=&quot;image-20220731110217800&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sample image를 확인한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;build-a-model&quot;&gt;&lt;strong&gt;Build a Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110226735.png&quot; alt=&quot;image-20220731110226735&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;간단한 CNN 모델을 구성한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-the-model&quot;&gt;&lt;strong&gt;Train the Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110234568.png&quot; alt=&quot;image-20220731110234568&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모델 training에 사용할 optimizer, loss, metric을 세팅한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fit&lt;/code&gt;을 실행하면 모델 학습이 진행되고, history 변수에는 epoch by epoch accuracy가 저장된다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluate-the-model&quot;&gt;&lt;strong&gt;Evaluate the Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110241758.png&quot; alt=&quot;image-20220731110241758&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Testset에 대해 학습된 모델의 정확도를 평가한다.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;save-the-model&quot;&gt;&lt;strong&gt;Save the Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110250111.png&quot; alt=&quot;image-20220731110250111&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tensorflow serving을 사용하기 위해서는 모델을 저장해야한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;launch-your-saved-model&quot;&gt;&lt;strong&gt;Launch Your Saved Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110257655.png&quot; alt=&quot;image-20220731110257655&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;bash script로 tensorflow model server를 세팅해준다. (port, 모델 이름, 모델 path)&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;send-an-inference-request&quot;&gt;&lt;strong&gt;Send an Inference Request&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110306735.png&quot; alt=&quot;image-20220731110306735&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Test set에 속하는 몇개의 이미지를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; 형식으로 변환한후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST&lt;/code&gt;로 서버에 request를 보낸다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;plot-predictions&quot;&gt;&lt;strong&gt;Plot Predictions&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110314567.png&quot; alt=&quot;image-20220731110314567&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;결과를 시각화하는 코드를 작성한다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-demo&quot;&gt;&lt;strong&gt;Results Demo&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731110323919.png&quot; alt=&quot;image-20220731110323919&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;결과가 정확하게 나온 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Google colab을 이용하여 tensorflow serving을 install하고 간단한 CNN 모델을 통해 실습을 진행한다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Model Serving Architecture</title><link href="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Model-Serving-Architecture/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Model Serving Architecture" /><published>2022-07-31T00:00:00+09:00</published><updated>2022-07-31T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/31/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Model%20Serving%20Architecture</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/31/Deploying-Machine-Learning-Models-in-Production-Model-Serving-Architecture/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-infrastructure-1&quot;&gt;&lt;strong&gt;ML Infrastructure (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165510452.png&quot; alt=&quot;image-20220731165510452&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model serving을 위한 infrastructure에 대해 알아볼 것이다.&lt;/li&gt;
  &lt;li&gt;크게 자체 구축한 on-premise 환경 또는 cloud 서비스를 이용하는 두가지 방법이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-infrastructure_on-premise-2&quot;&gt;&lt;strong&gt;ML Infrastructure_On-Premise (2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165518094.png&quot; alt=&quot;image-20220731165518094&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On-Premise는 소유한 회사가 하드웨어와 소프트웨어에 대한 전적인 통제 권한을 가질 수 있다.&lt;/li&gt;
  &lt;li&gt;따라서, 시시각각 변하는 다양한 요구 조건에 빠르게 대처할 수 있다는 장점이 존재한다.&lt;/li&gt;
  &lt;li&gt;하지만 하드웨어 인프라를 직접 구하고, 설치하고, 유지보수해야한다는 점에서 큰 비용이 생긴다.&lt;/li&gt;
  &lt;li&gt;일반적으로 규모의 경제를 실현할 수 있는 큰 규모의 회사들이 on-premise 환경을 구축하고 운영한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ml-infrastructure_cloud-3&quot;&gt;&lt;strong&gt;ML Infrastructure_Cloud (3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165526447.png&quot; alt=&quot;image-20220731165526447&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;규모가 작은 기업들은 이러한 인프라를 전문 기업에게 outsourcing 하는 방법이 있다.&lt;/li&gt;
  &lt;li&gt;아마존, 구글, 마이크로소프트와 같은 기업들이 cloud 서비스를 제공한다.&lt;/li&gt;
  &lt;li&gt;하드웨어를 직접 구축할 필요가 없고 해당 기업들이 하드웨어, 소프트웨어 전반을 관리 및 모니터링해준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving-1&quot;&gt;&lt;strong&gt;Model Serving (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165534694.png&quot; alt=&quot;image-20220731165534694&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인프라 선택뿐만 아니라 model serving 방법 또한 선택해야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving_on-premise-2&quot;&gt;&lt;strong&gt;Model Serving_On-Premise (2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165542350.png&quot; alt=&quot;image-20220731165542350&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On-premise에서는 원하는 model server를 선택하고 구축할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-serving_cloud3&quot;&gt;&lt;strong&gt;Model Serving_Cloud(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165549647.png&quot; alt=&quot;image-20220731165549647&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloud를 사용하는 경우 cloud vendor에서 제공하는 tool과 service를 사용할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;Google의 경우 automl을 비롯한 다양한 서비스를, amazon의 경우 sagemaker와 같은 서비스를 사용할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아니면 cloud에 VM을 활용하여 on-premise와 동일하게 직접 원하는 환경과 서비스를 구축할수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers-1&quot;&gt;&lt;strong&gt;Model Servers (1)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165556703.png&quot; alt=&quot;image-20220731165556703&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model server의 역할을 간단하게 알아보자.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_model-file2&quot;&gt;&lt;strong&gt;Model Servers_Model file(2)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165604454.png&quot; alt=&quot;image-20220731165604454&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델은 여러 버전으로 file system에 저장이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_model-server3&quot;&gt;&lt;strong&gt;Model Servers_Model server(3)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165612567.png&quot; alt=&quot;image-20220731165612567&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model server는 model을 file에서 읽고 인스턴스로 생성하여, client에게 제공하길 원하는 task에 대한 response를 한다.&lt;/li&gt;
  &lt;li&gt;예를들어 Mobilenet을 이용한 classification을 수행한다고 했을 때, 모델 서버는 input으로 주어진 이미지를 tensor 형태로 변형하고 이를 model에 전달하여 inference 결과를 산출한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_api4&quot;&gt;&lt;strong&gt;Model Servers_API(4)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165624486.png&quot; alt=&quot;image-20220731165624486&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model server는 client에게 REST 또는 gRPC 형태의 API를 제공하고 inference 결과를 client에게 return한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-servers_popular-model-server5&quot;&gt;&lt;strong&gt;Model Servers_Popular model server(5)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220731165635639.png&quot; alt=&quot;image-20220731165635639&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;유명한 model server에는 Tensorflow serving, Torchserve, KF serving, Triton이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving에 필요한 infrastructure의 종류와 model server에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Deployment Options</title><link href="http://localhost:4000/lecture/2022/07/30/Deploying-Machine-Learning-Models-in-Production-Deployment-Options/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Deployment Options" /><published>2022-07-30T00:00:00+09:00</published><updated>2022-07-30T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/30/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Deployment%20Options</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/30/Deploying-Machine-Learning-Models-in-Production-Deployment-Options/">&lt;hr /&gt;

&lt;h2 id=&quot;model-deployments&quot;&gt;&lt;strong&gt;Model Deployments&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095805422.png&quot; alt=&quot;image-20220730095805422&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델 배포는 데이터센터와 같이 대규모의 인프라 공간에 모델을 중앙집중형 서버에 배포하는 것과 각 사용자의 local device에 배포하는 두가지가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-in-huge-data-centers&quot;&gt;&lt;strong&gt;Running in Huge Data Centers&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095619218.png&quot; alt=&quot;image-20220730095619218&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터센터를 운영하면 cost를 줄이기 위한 노력을 수행한다.
    &lt;ul&gt;
      &lt;li&gt;Google과 같은 큰 기업들은 지속적으로 데이터센터의 resource utilization, application의 cost를 줄이려고 한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;constrained-environment-mobile-phone&quot;&gt;&lt;strong&gt;Constrained Environment: Mobile Phone&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095627775.png&quot; alt=&quot;image-20220730095627775&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;거대한 데이터센터 뿐만 아니라 핸드폰과 같은 제한된 모바일 환경에서도 모델을 serving하기도 한다.&lt;/li&gt;
  &lt;li&gt;모바일핸드폰의 GPU에는 데이터센터에서 사용되는 GPU보다 크기가 훨씬 작고 메모리 용량은 일반적으로 4GB가 넘지 않는다.&lt;/li&gt;
  &lt;li&gt;이러한 GPU를 우리가 배포한 application이 점유하는 것이 아니라 실행되고 있는 여러 application이 점유하게 된다.&lt;/li&gt;
  &lt;li&gt;또한 GPU를 이용하여 연산을 가속화하면 배터리가 빨리 닳게 되고 열이 발생해 application에 대한 인식이 좋지 않아질 것이다.&lt;/li&gt;
  &lt;li&gt;마지막으로, application의 용량이 커지면 다운로드 받기 꺼려질 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;restrictions-in-a-constrained-environment&quot;&gt;&lt;strong&gt;Restrictions in a Constrained Environment&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095635328.png&quot; alt=&quot;image-20220730095635328&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;낮은 메모리 용량, 프로세싱 파워, 배터리 용량과 같은 제한된 요건을 갖기 때문에 edge device에 큰 모델을 배포할 수 없다.&lt;/li&gt;
  &lt;li&gt;이러한 이유로 보통 서버에 모델을 배포하고 REST API를 통해 서비스를 하지만, latency가 매우 중요할 때는  적합하지 않다.
    &lt;ul&gt;
      &lt;li&gt;자율주행자동차는 실시간으로 그때그때 판단을 해야 하는데, 서버와 통신을 하게 되면 network 지연과 같은 문제로 큰 문제가 초래될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;prediction-latency-is-almost-always-important&quot;&gt;&lt;strong&gt;Prediction Latency is Almost Always Important&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095647690.png&quot; alt=&quot;image-20220730095647690&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;항상 latency 를 줄여 application의 response time을 줄여 user experience를 향상시켜라.
    &lt;ul&gt;
      &lt;li&gt;예외적인 경우는 prediction의 정확도가 훨씬 중요한 task를 수행할 때이다.&lt;/li&gt;
      &lt;li&gt;예시는 질병 분석이 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이때 model complexity, size와 같은 trade-off 관계에 있는 요소들도 고려해야 하며, 발생하는 cost도 고려해야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;choose-best-model-for-the-task&quot;&gt;&lt;strong&gt;Choose Best Model for the Task&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095656008.png&quot; alt=&quot;image-20220730095656008&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;언급한 constraints를 기준으로 최적의 모델을 선택해야한다.
    &lt;ul&gt;
      &lt;li&gt;Mobilenet은 모바일 환경을 위해 개발된 모델이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;other-strategies&quot;&gt;&lt;strong&gt;Other Strategies&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095702751.png&quot; alt=&quot;image-20220730095702751&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;프로파일링을 통해 병목지점을 파악하고,&lt;/li&gt;
  &lt;li&gt;특정 operation이 많은 시간을 차지한다는 것을 발견한다면, 해당 부분은 최적화해야 한다.&lt;/li&gt;
  &lt;li&gt;모델 자체를 최적화하여 더 빠르고 energy efficient한 모델을 만들수도 있며 특히 mobile 환경에서 중요하다.&lt;/li&gt;
  &lt;li&gt;사용하는 thread의 수를 늘리는 방법도 있다.
    &lt;ul&gt;
      &lt;li&gt;하지만 thread 수를 늘린다고 해서 무조건 성능이 좋아지는 것은 아니다.&lt;/li&gt;
      &lt;li&gt;무엇을 concurrent하게 실행하고 있는지 등에 따라 성능 차이가 발생한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;web-applications-for-users&quot;&gt;&lt;strong&gt;Web Applications for Users&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095709919.png&quot; alt=&quot;image-20220730095709919&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델에서 서비스를 받는 유저는 request를 web application을 통해한다.&lt;/li&gt;
  &lt;li&gt;Model은 API 서비스 형태로 존재하고, 이러한 과정을 돕는 다양한 web framework들이 존재한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;serving-systems-for-easy-deployment&quot;&gt;&lt;strong&gt;Serving systems for easy deployment&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095716511.png&quot; alt=&quot;image-20220730095716511&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델 서빙 시스템은 다양한 방식으로 모델 배포를 관리한다.
    &lt;ul&gt;
      &lt;li&gt;서버에 모델을 배포하여 서비스를 제공할 수 있으며,&lt;/li&gt;
      &lt;li&gt;Custom website가 필요하지 않고,&lt;/li&gt;
      &lt;li&gt;몇줄 안되는 코드로 배포가 가능하고,&lt;/li&gt;
      &lt;li&gt;Model update와 rollback을 편리하게 수행할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;clipper&quot;&gt;&lt;strong&gt;Clipper&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095723575.png&quot; alt=&quot;image-20220730095723575&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clipper는 UC berkely에서 만든 model serving 시스템이다.
    &lt;ul&gt;
      &lt;li&gt;다양한 모델 배포를 지원하며, restful API로 기존 application과 쉽게 통합할 수 있으며, 도커로 컨테이너화하여 자원 관리를 지원하며, latency setting을 할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-serving&quot;&gt;&lt;strong&gt;TensorFlow Serving&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095732320.png&quot; alt=&quot;image-20220730095732320&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tensorflow serving은 구글에서 만든 오픈소스이다.
    &lt;ul&gt;
      &lt;li&gt;텐서플로우 모델을 쉽게 배포할 수 있으며,&lt;/li&gt;
      &lt;li&gt;다른 타입의 모델도 배포할 수 있으며,&lt;/li&gt;
      &lt;li&gt;REST와 gRPC 프로토콜을 제공하며,&lt;/li&gt;
      &lt;li&gt;모델의 version management가 가능하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;advantages-of-serving-with-a-managed-service&quot;&gt;&lt;strong&gt;Advantages of Serving with a Managed Service&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220730095739374.png&quot; alt=&quot;image-20220730095739374&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google cloud와 같은 managed service를 사용하면 더 편할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;낮은 latency의 endpoint를 제공하고 대량의 batch를 처리할 수 있으며,&lt;/li&gt;
      &lt;li&gt;별도 환경 또는 클라우드에서 학습한 모델을 배포할 수 있으며,&lt;/li&gt;
      &lt;li&gt;traffic에 기반하여 자동으로 scaling을 수행하고&lt;/li&gt;
      &lt;li&gt;GPU/TPU와 같은 연산 가속기도 지원한다.
        &lt;ul&gt;
          &lt;li&gt;Google 외에도 MS, Amazon과 같은 기업들이 비슷한 서비스를 제공한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving을 위한 다양한 option들에 대해 알아본다.</summary></entry><entry><title type="html">Deploying Machine Learning Models in Production_Introduction to Model Serving Infrastructure</title><link href="http://localhost:4000/lecture/2022/07/29/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving-Infrastructure/" rel="alternate" type="text/html" title="Deploying Machine Learning Models in Production_Introduction to Model Serving Infrastructure" /><published>2022-07-29T00:00:00+09:00</published><updated>2022-07-29T00:00:00+09:00</updated><id>http://localhost:4000/lecture/2022/07/29/%5BDeploying%20Machine%20Learning%20Models%20in%20Production%5D%20Introduction%20to%20Model%20Serving%20Infrastructure</id><content type="html" xml:base="http://localhost:4000/lecture/2022/07/29/Deploying-Machine-Learning-Models-in-Production-Introduction-to-Model-Serving-Infrastructure/">&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizing-models-for-serving&quot;&gt;&lt;strong&gt;Optimizing Models for Serving&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225850339.png&quot; alt=&quot;image-20220729225850339&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;최근 모델의 정확도를 높이기 위해 model에 feature 수를 늘리는 등 모델 자체의 complexity가 커지고 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;정확도는 높아지지만 이로인해 prediction latency도 커지고 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;as-model-complexity-increases-cost-increases&quot;&gt;&lt;strong&gt;As Model Complexity Increases Cost Increases&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225858482.png&quot; alt=&quot;image-20220729225858482&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델 complexity의 증가는 비용을 초래한다.&lt;/li&gt;
  &lt;li&gt;GPU, TPU와 같은 하드웨어와 model registry, maintenance에 있어서의 비용이 포함된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;balancing-cost-and-complexity&quot;&gt;&lt;strong&gt;Balancing Cost and Complexity&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225907449.png&quot; alt=&quot;image-20220729225907449&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델의 정확도와 prediction 속도 간에는 trade-off 관계가 존재하며, 이 사이의 균형을 맞추는 것이 중요하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizing-and-satisficing-metrics&quot;&gt;&lt;strong&gt;Optimizing and Satisficing Metrics&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225915162.png&quot; alt=&quot;image-20220729225915162&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모델의 정확도를 측정하는 metric으로 accuracy, precision, recall이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반면, 연산상의 제약 조건과 관련된 metric으로는 latency, model size, GPU load가 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizing-and-satisficing-metrics-1&quot;&gt;&lt;strong&gt;Optimizing and Satisficing Metrics&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225927745.png&quot; alt=&quot;image-20220729225927745&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이와 같은 metric을 만족하는 방법은 우선 특정한 serving infrastructure에서 model의 complexity를 증가시키는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Latency와 같은 연산 상의 제약조건 metric threshold에(e.g. latency) 걸리는 순간까지 정확도를 높인다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이후 최종 결과를 평가하고 난 후에 정확도를 높이거나, infra를 증축하거나, 모델의 complexity를 줄이거나와 같은 대책을 마련하고 실행한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-of-accelerators-in-serving-infrastructure&quot;&gt;&lt;strong&gt;Use of Accelerators in Serving Infrastructure&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225935034.png&quot; alt=&quot;image-20220729225935034&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Accelerator를 통해 infrastructure를 최적화할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GPU의 경우는 training 가속화에 강점을 보이며,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TPU는 large batch size, complex model inference에 강점이 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;maintaining-input-feature-lookup&quot;&gt;&lt;strong&gt;Maintaining Input Feature Lookup&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225942225.png&quot; alt=&quot;image-20220729225942225&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prediction을 수행할 때에는 많은 feature들이 필요할 수 있다.&lt;/li&gt;
  &lt;li&gt;예를들어 음식 배달 시간을 예측한다고 할때, 주문 수, 현재 교통 상황, 처리되지 않은 주문 수와 같은 많은 정보들이 필요하다.&lt;/li&gt;
  &lt;li&gt;Prediction latency를 줄이기 위해서는 사용되는 대량의 정보를 data store에서 빠르게 읽을 수 있어야 한다.&lt;/li&gt;
  &lt;li&gt;이때 cache를 사용하면 필요한 정보를 빠르게 읽어 latency를 낮출 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nosql-databases-caching-and-feature-lookup&quot;&gt;&lt;strong&gt;NoSQL Databases: Caching and Feature Lookup&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Lecture/Introduction to Model Serving/Introduction to Model Serving Infrastructure/image-20220729225950122.png&quot; alt=&quot;image-20220729225950122&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NoSQL은 caching과 feature look up을 구현하기에 좋으며, 위와 같이 다양한 옵션들이 존재한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Jaeseo Yu</name></author><category term="Lecture" /><category term="mlops" /><category term="deep learning" /><category term="machine learning" /><category term="lecture summary" /><summary type="html">Model serving 인프라에서 고려해야할 점들에 대한 내용을 알아본다.</summary></entry></feed>